{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gopalkalpande/Practice-AI/blob/master/lstm_for_closing_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0f191f3e23f8b7293a64e6021c261253b71c6a3",
    "colab_type": "text",
    "id": "carYp1e5BD-z"
   },
   "source": [
    "# Stock value prediction from Open, High, Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c68860170afd744bf4169a9a4b74dc81f8749242",
    "colab_type": "text",
    "collapsed": true,
    "id": "mMurv2asBD-0"
   },
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "7ecd49c3ce39645feb4c3ffcff0b9198a7ebd098",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_8X85MAdBD-1",
    "outputId": "a2f48c29-8159-488b-d41d-7710293a3f80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "EVH7n0MXBtLT",
    "outputId": "8d7bdfde-2751-4018-da66-edd275727f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22d37de4dcd90089be693aa65bda1a1081f8068e",
    "colab_type": "text",
    "id": "ngTTsbptBD-6"
   },
   "source": [
    "# Input parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c513bc304e26718051f5e80dd0a9e29b0fa1f15c",
    "colab": {},
    "colab_type": "code",
    "id": "HgFF4H3UBD-7"
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "seq_len = 22\n",
    "rate = 0.4\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be5579ea9e65f1cf48636060b2af8722eddab66b",
    "colab_type": "text",
    "collapsed": true,
    "id": "NkGBxB_XBD-9"
   },
   "source": [
    "# 1. Download data and normalize it\n",
    "Data since 1950 to today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "2d0d0333cb172a78e588feaa694f2619438f4d95",
    "colab": {},
    "colab_type": "code",
    "id": "nrcsMo9IBD--"
   },
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, normalize=True):\n",
    "    start = datetime.datetime(2013, 1, 1)\n",
    "    end = datetime.datetime(2018,12,31)\n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    df.drop(['Volume', 'Adj Close'], 1, inplace=True)\n",
    "    \n",
    "    if normalize:        \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n",
    "        df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n",
    "        df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n",
    "        df['Close'] = min_max_scaler.fit_transform(df['Close'].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "c4bc6fa7f89ff696cb5c01da8edab61a357317c8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "sl5GwTiZBD_A",
    "outputId": "50192715-734f-4226-e278-1a72ad9ab8cd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>5.44500</td>\n",
       "      <td>5.32000</td>\n",
       "      <td>5.43625</td>\n",
       "      <td>5.35750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>5.47875</td>\n",
       "      <td>5.36375</td>\n",
       "      <td>5.38250</td>\n",
       "      <td>5.43375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>5.36750</td>\n",
       "      <td>5.29250</td>\n",
       "      <td>5.36750</td>\n",
       "      <td>5.33500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>5.41500</td>\n",
       "      <td>5.33500</td>\n",
       "      <td>5.33750</td>\n",
       "      <td>5.41125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>5.35000</td>\n",
       "      <td>5.30375</td>\n",
       "      <td>5.31125</td>\n",
       "      <td>5.31500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               High      Low     Open    Close\n",
       "Date                                          \n",
       "2013-01-02  5.44500  5.32000  5.43625  5.35750\n",
       "2013-01-03  5.47875  5.36375  5.38250  5.43375\n",
       "2013-01-04  5.36750  5.29250  5.36750  5.33500\n",
       "2013-01-07  5.41500  5.33500  5.33750  5.41125\n",
       "2013-01-08  5.35000  5.30375  5.31125  5.31500"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.datetime(2013, 1, 1)\n",
    "end = datetime.datetime(2018,12,31)\n",
    "df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "df.drop(['Volume', 'Adj Close'], 1, inplace=True)\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wLXwDeQzfvza",
    "outputId": "0ae8f26c-b987-457f-d031-ee6a1ec561a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1510, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "253eddeff446a6b0b0494a2dd1063c55a06e6cac",
    "colab": {},
    "colab_type": "code",
    "id": "_VAXROePBD_D"
   },
   "outputs": [],
   "source": [
    "df = get_stock_data(stock_name, normalize=True)\n",
    "#df = pd.read_csv('/content/drive/My Drive/data/lstm/nasdaq100_padding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4ba620c3232a01de577de152f3e3f6156d05212f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "5b3MkOZfBD_F",
    "outputId": "4a949b1e-48af-44e2-96a8-f827a33dee4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>0.087642</td>\n",
       "      <td>0.080521</td>\n",
       "      <td>0.086870</td>\n",
       "      <td>0.079921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>0.093558</td>\n",
       "      <td>0.088242</td>\n",
       "      <td>0.077365</td>\n",
       "      <td>0.093278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>0.074058</td>\n",
       "      <td>0.075667</td>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.075980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>0.082384</td>\n",
       "      <td>0.083168</td>\n",
       "      <td>0.069408</td>\n",
       "      <td>0.089337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>0.070990</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.064766</td>\n",
       "      <td>0.072476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                High       Low      Open     Close\n",
       "Date                                              \n",
       "2013-01-02  0.087642  0.080521  0.086870  0.079921\n",
       "2013-01-03  0.093558  0.088242  0.077365  0.093278\n",
       "2013-01-04  0.074058  0.075667  0.074713  0.075980\n",
       "2013-01-07  0.082384  0.083168  0.069408  0.089337\n",
       "2013-01-08  0.070990  0.077653  0.064766  0.072476"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "07dbf6a915677dce195b97cad7df4b9b33acb271",
    "colab": {},
    "colab_type": "code",
    "id": "Zu7Jo-5eBD_I"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/My Drive/abc/dataset_unnormalized.csv', sep=',', encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67cb16efbd63df9d0fe4e8d8f2c79426bcde2752",
    "colab_type": "text",
    "id": "4UokwteiBD_L"
   },
   "source": [
    "# 2. Plot out the Normalized Adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "99295b0d378333819b2e14e79a1ca58669f0894f",
    "colab": {},
    "colab_type": "code",
    "id": "ZBKKvlufBD_L"
   },
   "outputs": [],
   "source": [
    "def plot_stock(df):\n",
    "    df = get_stock_data(stock_name, normalize=True)\n",
    "    print(df.head())\n",
    "    plt.plot(df['Close'], color='red', label='Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "ceca2c0b16b6b16161b26e7ef1d6289d5be4d34e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "Pu8TmDDYBD_O",
    "outputId": "d3b8d70c-dd68-4ffb-c71d-19fb560500a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                High       Low      Open     Close\n",
      "Date                                              \n",
      "2013-01-02  0.087642  0.080521  0.086870  0.079921\n",
      "2013-01-03  0.093558  0.088242  0.077365  0.093278\n",
      "2013-01-04  0.074058  0.075667  0.074713  0.075980\n",
      "2013-01-07  0.082384  0.083168  0.069408  0.089337\n",
      "2013-01-08  0.070990  0.077653  0.064766  0.072476\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRVGUAkTFXVEUpQBRcVcURSlA/j9SClOB7gtv6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mP_n1bWo8u9"
   },
   "outputs": [],
   "source": [
    "\n",
    "rolmean = df.Close.rolling(12).mean()\n",
    "rolstd = df.Close.rolling(12).std() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4Ypzaa4sSCt"
   },
   "outputs": [],
   "source": [
    "rolmean1 = df.Close.rolling(5).mean()\n",
    "rolstd1 = df.Close.rolling(5).std() \n",
    "df['moving_average'] = rolmean1\n",
    "df['moving_variance'] = rolstd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PUweFrys4Xp"
   },
   "outputs": [],
   "source": [
    "df1 = df\n",
    "df1 = df1.drop(['High', 'High', 'Open'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "aqHRStGdsbFU",
    "outputId": "c2db23c4-93f5-499f-de63-e5d6115fafb0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>moving_average</th>\n",
       "      <th>moving_variance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>0.079921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>0.093278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-04</th>\n",
       "      <td>0.075980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-07</th>\n",
       "      <td>0.089337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-08</th>\n",
       "      <td>0.072476</td>\n",
       "      <td>0.082198</td>\n",
       "      <td>0.008833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-09</th>\n",
       "      <td>0.075980</td>\n",
       "      <td>0.081410</td>\n",
       "      <td>0.009253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-10</th>\n",
       "      <td>0.103788</td>\n",
       "      <td>0.083512</td>\n",
       "      <td>0.013041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-11</th>\n",
       "      <td>0.284870</td>\n",
       "      <td>0.125290</td>\n",
       "      <td>0.090058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-14</th>\n",
       "      <td>0.278520</td>\n",
       "      <td>0.163127</td>\n",
       "      <td>0.108938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-15</th>\n",
       "      <td>0.258156</td>\n",
       "      <td>0.200263</td>\n",
       "      <td>0.101720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-16</th>\n",
       "      <td>0.265382</td>\n",
       "      <td>0.238143</td>\n",
       "      <td>0.075841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-17</th>\n",
       "      <td>0.282899</td>\n",
       "      <td>0.273965</td>\n",
       "      <td>0.011652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-18</th>\n",
       "      <td>0.284651</td>\n",
       "      <td>0.273922</td>\n",
       "      <td>0.011601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-22</th>\n",
       "      <td>0.280272</td>\n",
       "      <td>0.274272</td>\n",
       "      <td>0.011799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-23</th>\n",
       "      <td>0.302387</td>\n",
       "      <td>0.283118</td>\n",
       "      <td>0.013194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Close  moving_average  moving_variance\n",
       "Date                                                 \n",
       "2013-01-02  0.079921             NaN              NaN\n",
       "2013-01-03  0.093278             NaN              NaN\n",
       "2013-01-04  0.075980             NaN              NaN\n",
       "2013-01-07  0.089337             NaN              NaN\n",
       "2013-01-08  0.072476        0.082198         0.008833\n",
       "2013-01-09  0.075980        0.081410         0.009253\n",
       "2013-01-10  0.103788        0.083512         0.013041\n",
       "2013-01-11  0.284870        0.125290         0.090058\n",
       "2013-01-14  0.278520        0.163127         0.108938\n",
       "2013-01-15  0.258156        0.200263         0.101720\n",
       "2013-01-16  0.265382        0.238143         0.075841\n",
       "2013-01-17  0.282899        0.273965         0.011652\n",
       "2013-01-18  0.284651        0.273922         0.011601\n",
       "2013-01-22  0.280272        0.274272         0.011799\n",
       "2013-01-23  0.302387        0.283118         0.013194"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1.drop(['Low'], axis = 1)\n",
    "df1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "1aKifSkNpfem",
    "outputId": "2c1818b7-793f-47b1-d67c-5b6fa59ee50a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAF1CAYAAAB76AIVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEIIIYQQohr7/+yZzixL39BYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1080x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6\n",
    "orig = plt.plot(df.Close, color='blue',label='Original')\n",
    "mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Rolling Mean & Rolling Standard Deviation')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhPLZ0t7xicc"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ogjYtMKZxiU3",
    "outputId": "f8ba7165-aa0a-4283-a5d4-485586e391b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic                   -2.467311\n",
      "p-value                           0.123632\n",
      "#Lags Used                        0.000000\n",
      "Number of Observations Used    1509.000000\n",
      "Critical Value (1%)              -3.434691\n",
      "Critical Value (5%)              -2.863457\n",
      "Critical Value (10%)             -2.567791\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "dftest = adfuller(df.Close, autolag='AIC')\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "print(dfoutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ce2a9c708b08ab60c224cda4b1beb2f7fbdec5b",
    "colab_type": "text",
    "id": "QlFEdUy0BD_S"
   },
   "source": [
    "# 3. Set last day Close as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c7adba73a191968dc89f773537452b472368d043",
    "colab": {},
    "colab_type": "code",
    "id": "Ya3_-LFGBD_S"
   },
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    \n",
    "    train = result[:int(row), :] # 90% date\n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "    \n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1] \n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "11d1886f997111cabf11bbadad86129554595739",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XSCX3VpcBD_V",
    "outputId": "b234bbbf-5829-425b-f029-9443d69f8162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn import model_selection\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.model_selection import TimeSeriesSplit\\n\\ndef load_data_1(stock, split_point):\\n    amount_of_features = len(stock.columns)\\n    stock = np.array(stock)\\n    X = stock[:,:-1]\\n    y = stock[:,-1]\\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = split_point, shuffle = False)\\n    \\n    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\\n    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \\n    \\n    return [X_train, y_train, X_test, y_test]  '"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def load_data_1(stock, split_point):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    stock = np.array(stock)\n",
    "    X = stock[:,:-1]\n",
    "    y = stock[:,-1]\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = split_point, shuffle = False)\n",
    "    \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "    \n",
    "    return [X_train, y_train, X_test, y_test]  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "b1f7c6d93296a90ccc584cf2bac88a23c5523743",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "i742m_2GBD_Y",
    "outputId": "ce52dc25-8257-4d8f-8cce-165880f0f71b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "a37d326c2a059f116c52d146eb0d854d69753111",
    "colab": {},
    "colab_type": "code",
    "id": "Seq78i6GBD_d"
   },
   "outputs": [],
   "source": [
    "# X_train, y_train, X_test, y_test = load_data_1(df, .25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "db9ad8f83f20e5767893e29f99e248fba398d4c5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dfA_8kXKBD_f",
    "outputId": "cc64285e-90da-4332-b868-b79c392bbc41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1338, 22, 4), (149, 22, 4), (1338,), (149,))"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "b5a03bfb89dc059bafa5cb72f024c8d5418e3149",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ceP-a6OABD_k",
    "outputId": "b1b500aa-fea7-4c61-dc04-f15826d36b58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.itemsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "2e562342dc33e614960a9e4888dab99ff322591a",
    "colab": {},
    "colab_type": "code",
    "id": "1XBdvFbkBD_p"
   },
   "outputs": [],
   "source": [
    "# X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "1c7ade7662188d511228b096b22d7c9c5709ff5a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9XtkyLmnBD_v",
    "outputId": "596ca9d1-f226-4536-ac3e-24bf95d35bbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 22, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c825abdbd2bfda828b3cff49bad3256e83a3a299",
    "colab": {},
    "colab_type": "code",
    "id": "SXg_hPdOBD_1"
   },
   "outputs": [],
   "source": [
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16feb2617e21604d98a5ae720d5f840a172ebbe9",
    "colab_type": "text",
    "collapsed": true,
    "id": "utOeyoMlBD_6"
   },
   "source": [
    "# 4. Buidling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "7a6f779c37629d5cc04dabac0419bfabb951f319",
    "colab": {},
    "colab_type": "code",
    "id": "7cassPH6BD_6"
   },
   "outputs": [],
   "source": [
    "def build_model2(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(rate, noise_shape=None, seed=None))\n",
    "    \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(rate, noise_shape=None, seed=None))\n",
    "        \n",
    "    model.add(LSTM(neurons[2], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(rate, noise_shape=None, seed=None))\n",
    "        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[4],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    # adam = keras.optimizers.Adam(decay=0.2)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5edf1da1a967c021ea97f610c9b1d6f47e3956f",
    "colab_type": "text",
    "id": "hd3V7RqzBD_8"
   },
   "source": [
    "# 6. Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "220c9fc2ad9c4955b5736d85fa0fd18cadcb6a3d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "colab_type": "code",
    "id": "uHcYYi6ABD_8",
    "outputId": "c58d3479-b129-4bcc-aad3-84ba457ddb1a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0623 16:05:27.598786 140245755078528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0623 16:05:27.643777 140245755078528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0623 16:05:27.652926 140245755078528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0623 16:05:27.903298 140245755078528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0623 16:05:27.911127 140245755078528 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0623 16:05:28.464915 140245755078528 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_len = 22\n",
    "rate = 0.4\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 256, 128, 32, 1]\n",
    "epochs = 300\n",
    "\n",
    "model = build_model2(shape, neurons, rate)\n",
    "# layers = [4, 22, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "cc613aaf7a13b63f2c883f4dc46e5a2df1e41ecb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "AMIpDYwABD__",
    "outputId": "ef8254af-6f92-4dcf-ea29-9caa07f7d4f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "55f429403692a0b0fd4360d4a5a628f9585a03f8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10718
    },
    "colab_type": "code",
    "id": "1D0F5cn1BEAE",
    "outputId": "c3f2d42a-79a0-41f5-80f5-8a42140c0c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 4s 3ms/step - loss: 0.2535 - acc: 8.3056e-04 - val_loss: 0.3609 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.1939 - acc: 8.3056e-04 - val_loss: 0.1517 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0570 - acc: 8.3056e-04 - val_loss: 0.0124 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0495 - acc: 0.0000e+00 - val_loss: 0.0101 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0409 - acc: 0.0000e+00 - val_loss: 0.0426 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0373 - acc: 8.3056e-04 - val_loss: 0.0720 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0415 - acc: 8.3056e-04 - val_loss: 0.0619 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0373 - acc: 8.3056e-04 - val_loss: 0.0369 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0346 - acc: 0.0000e+00 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0373 - acc: 0.0000e+00 - val_loss: 0.0234 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0352 - acc: 0.0000e+00 - val_loss: 0.0371 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0339 - acc: 8.3056e-04 - val_loss: 0.0466 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0340 - acc: 8.3056e-04 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0320 - acc: 0.0000e+00 - val_loss: 0.0269 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 351us/step - loss: 0.0288 - acc: 8.3056e-04 - val_loss: 0.0203 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0258 - acc: 8.3056e-04 - val_loss: 0.0142 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0211 - acc: 8.3056e-04 - val_loss: 0.0119 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0180 - acc: 8.3056e-04 - val_loss: 0.0081 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0151 - acc: 8.3056e-04 - val_loss: 0.0079 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 339us/step - loss: 0.0138 - acc: 8.3056e-04 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0111 - acc: 8.3056e-04 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0097 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 344us/step - loss: 0.0088 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0081 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0086 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0071 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0075 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0068 - acc: 8.3056e-04 - val_loss: 0.0068 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0060 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 335us/step - loss: 0.0068 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0065 - acc: 8.3056e-04 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0061 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0061 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0055 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 338us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 337us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 345us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 338us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 350us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 340us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 338us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 337us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.8123e-04 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.8812e-04 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.5927e-04 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.4722e-04 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.3818e-04 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.8381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.0791e-04 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.1449e-04 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.3235e-04 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.9201e-04 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.5328e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.0837e-04 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.8142e-04 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.0452e-04 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.6856e-04 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.1925e-04 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.4131e-04 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 335us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 7.3547e-04 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.9068e-04 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.7367e-04 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 340us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.5731e-04 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 342us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.8350e-04 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.3813e-04 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.5952e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 337us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.6938e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.1556e-04 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.7745e-04 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.7670e-04 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.1490e-04 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.3533e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 335us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.5691e-04 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.4983e-04 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.0013e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 7.1975e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.1020e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.6238e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.5194e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.0958e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.1425e-04 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.1197e-04 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.0791e-04 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.0267e-04 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.6873e-04 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.6828e-04 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 5.9797e-04 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.9991e-04 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.7687e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.2339e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.9382e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.5088e-04 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.8670e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.5723e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.3962e-04 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.0230e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.6547e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.4054e-04 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.9400e-04 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.9713e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.4790e-04 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.5811e-04 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 5.9196e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.6375e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.0368e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.9548e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.9841e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.1716e-04 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.0911e-04 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.4880e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.5381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.4261e-04 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.4729e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.8735e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.8802e-04 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.6798e-04 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.7606e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.0546e-04 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.1506e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.6218e-04 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.8306e-04 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.2973e-04 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.6262e-04 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.8703e-04 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.8606e-04 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.8012e-04 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.1500e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.1059e-04 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.3133e-04 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.0559e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 5.5522e-04 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.4976e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.5349e-04 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.1246e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 346us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.0424e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 339us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.1369e-04 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.4934e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.9877e-04 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 341us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.0003e-04 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.2978e-04 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.0277e-04 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.1255e-04 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 5.8417e-04 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.3490e-04 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.0178e-04 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.1093e-04 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.2391e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.1878e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 5.4487e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.2896e-04 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.9522e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 5.4697e-04 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 8.2955e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 6.5028e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.6395e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.9561e-04 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd937a9fd0>"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae8b76a4c41e2bd313e1ef7709977113f0ad11d5",
    "colab_type": "text",
    "id": "vnJTyLcdBEAH"
   },
   "source": [
    "# 7. Result on training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "61c52632efb4610eb4baadc2b96235bd2bcca9d9",
    "colab": {},
    "colab_type": "code",
    "id": "IvnJ54HrBEAH"
   },
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "f0ca7a987a25dc491f647de2c1c18b6b3e3d4f32",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "wxsfesQ5BEAJ",
    "outputId": "d100a751-309c-4128-e4a6-a5860bd072ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00095 MSE (0.03 RMSE)\n",
      "Test Score: 0.00434 MSE (0.07 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0009497975161777996, 0.0043373436654524115)"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "feef2d9990191d169de1ff20703122c85dc8b290",
    "colab_type": "text",
    "id": "R_fcPIDsBEAM"
   },
   "source": [
    "# 8. Prediction vs Real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "3d6c9d7ae50968908832763bf3086a1301926fe0",
    "colab": {},
    "colab_type": "code",
    "id": "gO9aUmBtBEAM"
   },
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "52c5d7561a9acadfccc7e144ca8c07d01efbf42d",
    "colab": {},
    "colab_type": "code",
    "id": "utmLWXVZBEAP"
   },
   "outputs": [],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "53c63509d153aa95bdaa5bf2151537e77c46ddb4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mpH2VBgXBEAR",
    "outputId": "4dc96129-3619-4b7c-fba9-448c353a8f1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "262eef0dd5562c47058bc13ef4ffc6e5fd96a4b3",
    "colab": {},
    "colab_type": "code",
    "id": "JvL1al4hBEAT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed8f6d54508a9d825d261e6dee70fa540dbc801c",
    "colab_type": "text",
    "id": "LOLp8F2hBEAV"
   },
   "source": [
    "# 9. Plot out prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "22d9f96216c52e800be6578b0ce5764a1ae8a4b4",
    "colab": {},
    "colab_type": "code",
    "id": "cpmy9TSFBEAV"
   },
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value):\n",
    "    start = datetime.datetime(2000, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    \n",
    "    df = df['Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "0b76baa9972763494f5c7ccb14a8abdd1bb492ee",
    "colab": {},
    "colab_type": "code",
    "id": "SIR4fItrBEAX"
   },
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('Days')\n",
    "    plt2.ylabel('Close')\n",
    "    plt2.show()\n",
    "    return newp,newy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "76e971ce5eae84237223d6016f00bfd601c1f06b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "NNUFx4UJBEAZ",
    "outputId": "7fbdfa92-d3dc-4f9d-e7e1-d65358f20947"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newp,newy_test = plot_result(stock_name, p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "7a2e2e040380fcf3d59dd99be699513b10aba32d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 47
    },
    "colab_type": "code",
    "id": "ScGkOw1EBEAb",
    "outputId": "3fb7c07d-3cc0-416f-92e9-c32558f91420"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [pred, actual]\n",
       "Index: []"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_pred = pd.DataFrame(columns=['pred', 'actual'])\n",
    "my_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "dc9de69c2297e5b31d6d19a25ec142a832c3da0c",
    "colab": {},
    "colab_type": "code",
    "id": "NlhBioRsBEAe"
   },
   "outputs": [],
   "source": [
    "my_pred['pred'] = newp.tolist()\n",
    "my_pred['actual'] = newy_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "749c8d0f093736a9f0734d3febda90eb1de27c19",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "MFRnDLo0BEAg",
    "outputId": "8cf046f6-90da-44d3-d9ce-663bafcdfa25",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8.250595092773438]</td>\n",
       "      <td>[8.418510250096968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[8.300057411193848]</td>\n",
       "      <td>[8.544661442008255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[8.251545906066895]</td>\n",
       "      <td>[8.58971666674878]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[8.235519409179688]</td>\n",
       "      <td>[8.67982539754874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[8.271662712097168]</td>\n",
       "      <td>[8.814987634408132]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pred               actual\n",
       "0  [8.250595092773438]  [8.418510250096968]\n",
       "1  [8.300057411193848]  [8.544661442008255]\n",
       "2  [8.251545906066895]   [8.58971666674878]\n",
       "3  [8.235519409179688]   [8.67982539754874]\n",
       "4  [8.271662712097168]  [8.814987634408132]"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "86a9c6078d88b43634823c0f584ae0c5f0e6f6e0",
    "colab": {},
    "colab_type": "code",
    "id": "sHJ8Fg45BEAi"
   },
   "outputs": [],
   "source": [
    "my_pred.to_csv('LSTM_Stock_prediction_1.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73fa1f3d9fa0d2d3fd4116d8b13ab59c996ec641",
    "colab_type": "text",
    "id": "bOU27Kz2BEAj"
   },
   "source": [
    "# 10. Save for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "7369c2b024dad6b028b6da6dd784437809a3726f",
    "colab": {},
    "colab_type": "code",
    "id": "Re0ljuVJBEAk"
   },
   "outputs": [],
   "source": [
    "model.save('LSTM_Stock_prediction-1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "852de06713751347ae2da9a17e09d33905aeede2",
    "colab_type": "text",
    "id": "r0-b8trfBEAm"
   },
   "source": [
    "# Part 2. Fine tune model\n",
    "# 11. Function to load data, train model and see score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "0a335d50d7bf8f26f580764b57fc5c7afde021fa",
    "colab": {},
    "colab_type": "code",
    "id": "Q2Fkl7mNBEAn"
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 256, 128, 32, 1]\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4a1b2ff48bcc902d65771a1bcac9d102c39467f9",
    "colab": {},
    "colab_type": "code",
    "id": "2MzTab5KBEAt"
   },
   "outputs": [],
   "source": [
    "def quick_measure(stock_name, seq_len, rate, shape, neurons, epochs):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model2(shape, neurons, rate)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80155c8821f222dc6af85f0b4f2d1f4137025388",
    "colab_type": "text",
    "id": "mlOGggwZBEAw"
   },
   "source": [
    "# 12. Fine tune hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f03870cf247ef451eaa3d8d3af479859839822ac",
    "colab_type": "text",
    "id": "rqSGO7uPBEAx"
   },
   "source": [
    "12.1 Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d20345db79ab3f561e2b0a591d72ada93a4f1324",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 78167
    },
    "colab_type": "code",
    "id": "AYjvoGDvBEAy",
    "outputId": "db9900fa-c667-47b9-c276-d7d038326504"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 4s 4ms/step - loss: 0.2544 - acc: 8.3056e-04 - val_loss: 0.3736 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.2059 - acc: 8.3056e-04 - val_loss: 0.1861 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0773 - acc: 8.3056e-04 - val_loss: 0.0346 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0402 - acc: 8.3056e-04 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0502 - acc: 0.0000e+00 - val_loss: 0.0248 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0359 - acc: 8.3056e-04 - val_loss: 0.0616 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 343us/step - loss: 0.0400 - acc: 8.3056e-04 - val_loss: 0.0704 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0401 - acc: 8.3056e-04 - val_loss: 0.0526 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0356 - acc: 8.3056e-04 - val_loss: 0.0296 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0356 - acc: 8.3056e-04 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0373 - acc: 0.0000e+00 - val_loss: 0.0250 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0348 - acc: 0.0000e+00 - val_loss: 0.0394 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0341 - acc: 8.3056e-04 - val_loss: 0.0462 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0334 - acc: 8.3056e-04 - val_loss: 0.0381 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0323 - acc: 8.3056e-04 - val_loss: 0.0246 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0313 - acc: 8.3056e-04 - val_loss: 0.0205 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0273 - acc: 8.3056e-04 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0254 - acc: 8.3056e-04 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0220 - acc: 8.3056e-04 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0200 - acc: 8.3056e-04 - val_loss: 0.0112 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0173 - acc: 8.3056e-04 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0144 - acc: 8.3056e-04 - val_loss: 0.0115 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0139 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0119 - acc: 8.3056e-04 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0110 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0099 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0094 - acc: 8.3056e-04 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0083 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0075 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0069 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0061 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0060 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0061 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.7939e-04 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.6540e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.4356e-04 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.5944e-04 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.5738e-04 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.5448e-04 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.9164e-04 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.9762e-04 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.4458e-04 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.4296e-04 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.8605e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.4514e-04 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.8672e-04 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.2010e-04 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.1688e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.6107e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.0597e-04 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.3762e-04 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.7950e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.9951e-04 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.2094e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.2145e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.4225e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.1660e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.7398e-04 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.8607e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 7.1861e-04 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.7769e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.4659e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.6443e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.7176e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.5278e-04 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.4363e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.1529e-04 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.7682e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.9232e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.5214e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.3557e-04 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.1791e-04 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.2368e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.3907e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.8896e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.8174e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.2214e-04 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.5789e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.1023e-04 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.5823e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.0041e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.1143e-04 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.6197e-04 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.8586e-04 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.8651e-04 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.6209e-04 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.1629e-04 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.7021e-04 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.0676e-04 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.5580e-04 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.8410e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.8484e-04 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.1711e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.7419e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.6550e-04 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.9704e-04 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.4512e-04 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.4542e-04 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6277e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.6910e-04 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.0848e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.9419e-04 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.9431e-04 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.7829e-04 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.8439e-04 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.8496e-04 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.6256e-04 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.2662e-04 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.8459e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.0273e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.3108e-04 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.5968e-04 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.1874e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.9565e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.8947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.8735e-04 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.3661e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00125 MSE (0.04 RMSE)\n",
      "Test Score: 0.00841 MSE (0.09 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_39 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 5s 4ms/step - loss: 0.2555 - acc: 8.3056e-04 - val_loss: 0.3785 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.2123 - acc: 8.3056e-04 - val_loss: 0.2042 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0885 - acc: 8.3056e-04 - val_loss: 0.0490 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0382 - acc: 8.3056e-04 - val_loss: 0.0079 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0522 - acc: 0.0000e+00 - val_loss: 0.0198 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0370 - acc: 8.3056e-04 - val_loss: 0.0529 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0380 - acc: 8.3056e-04 - val_loss: 0.0658 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0404 - acc: 8.3056e-04 - val_loss: 0.0535 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0363 - acc: 8.3056e-04 - val_loss: 0.0337 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0360 - acc: 0.0000e+00 - val_loss: 0.0255 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0367 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0349 - acc: 8.3056e-04 - val_loss: 0.0450 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0355 - acc: 8.3056e-04 - val_loss: 0.0414 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0337 - acc: 8.3056e-04 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0324 - acc: 8.3056e-04 - val_loss: 0.0246 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0302 - acc: 8.3056e-04 - val_loss: 0.0235 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0269 - acc: 8.3056e-04 - val_loss: 0.0201 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0247 - acc: 8.3056e-04 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0224 - acc: 8.3056e-04 - val_loss: 0.0196 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0198 - acc: 8.3056e-04 - val_loss: 0.0099 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0164 - acc: 8.3056e-04 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0139 - acc: 8.3056e-04 - val_loss: 0.0086 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0116 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0103 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0087 - acc: 8.3056e-04 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0088 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0074 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0073 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0061 - acc: 8.3056e-04 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0066 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0065 - acc: 8.3056e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0074 - acc: 8.3056e-04 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0065 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0060 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.2422e-04 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.9237e-04 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.8849e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.9016e-04 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.8707e-04 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 8.4930e-04 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.0958e-04 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.8706e-04 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.7658e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.0391e-04 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.9473e-04 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.9845e-04 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.1085e-04 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.6591e-04 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.6524e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.0196e-04 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.2970e-04 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.7947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.4804e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.0777e-04 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.1967e-04 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.0359e-04 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.4739e-04 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.5880e-04 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.0716e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.9570e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.2296e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.7797e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 7.0298e-04 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.5595e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.8675e-04 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.7271e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.0519e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.8898e-04 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.9314e-04 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.5984e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.9455e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.5220e-04 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.6980e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.4628e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.9530e-04 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.3560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.0369e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.9672e-04 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.0649e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.4759e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.3646e-04 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.7666e-04 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 6.6718e-04 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6709e-04 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.2242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.8950e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.1909e-04 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.4290e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.6609e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.5163e-04 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.3148e-04 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.7961e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.0565e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.7824e-04 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.3630e-04 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.8691e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.9389e-04 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.6278e-04 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 7.0390e-04 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 6.0293e-04 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.5283e-04 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.4754e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.4687e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.5992e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.9401e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 341us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 7.4133e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.3273e-04 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.4108e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00121 MSE (0.03 RMSE)\n",
      "Test Score: 0.00720 MSE (0.08 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_44 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 6s 5ms/step - loss: 0.2538 - acc: 8.3056e-04 - val_loss: 0.3683 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.1985 - acc: 8.3056e-04 - val_loss: 0.1595 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0612 - acc: 8.3056e-04 - val_loss: 0.0172 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0480 - acc: 0.0000e+00 - val_loss: 0.0093 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0444 - acc: 0.0000e+00 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0381 - acc: 8.3056e-04 - val_loss: 0.0709 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0418 - acc: 8.3056e-04 - val_loss: 0.0633 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0380 - acc: 8.3056e-04 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0352 - acc: 8.3056e-04 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0364 - acc: 0.0000e+00 - val_loss: 0.0242 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0358 - acc: 0.0000e+00 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0342 - acc: 8.3056e-04 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0345 - acc: 8.3056e-04 - val_loss: 0.0421 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0331 - acc: 8.3056e-04 - val_loss: 0.0313 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0314 - acc: 8.3056e-04 - val_loss: 0.0252 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0289 - acc: 8.3056e-04 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0260 - acc: 8.3056e-04 - val_loss: 0.0187 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0230 - acc: 8.3056e-04 - val_loss: 0.0168 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0200 - acc: 8.3056e-04 - val_loss: 0.0130 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0166 - acc: 8.3056e-04 - val_loss: 0.0113 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0148 - acc: 8.3056e-04 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0120 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0100 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0088 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0090 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0079 - acc: 8.3056e-04 - val_loss: 0.0080 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0078 - acc: 8.3056e-04 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0076 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0070 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0069 - acc: 8.3056e-04 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0065 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0072 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0068 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0057 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0068 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.9588e-04 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.5788e-04 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.8615e-04 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.6118e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.9078e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.6193e-04 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.2736e-04 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.7219e-04 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.8979e-04 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.2830e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.1743e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.1342e-04 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.9248e-04 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.3803e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.0595e-04 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.3645e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.2195e-04 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.9448e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.6515e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.9281e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.1167e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00236 MSE (0.05 RMSE)\n",
      "Test Score: 0.01594 MSE (0.13 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_45 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_47 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 7s 5ms/step - loss: 0.2525 - acc: 8.3056e-04 - val_loss: 0.3560 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.1876 - acc: 8.3056e-04 - val_loss: 0.1462 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0588 - acc: 8.3056e-04 - val_loss: 0.0194 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0436 - acc: 0.0000e+00 - val_loss: 0.0094 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0445 - acc: 0.0000e+00 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0363 - acc: 0.0000e+00 - val_loss: 0.0668 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0410 - acc: 8.3056e-04 - val_loss: 0.0610 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0374 - acc: 8.3056e-04 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0352 - acc: 8.3056e-04 - val_loss: 0.0217 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0372 - acc: 0.0000e+00 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0345 - acc: 0.0000e+00 - val_loss: 0.0358 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0341 - acc: 8.3056e-04 - val_loss: 0.0445 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0338 - acc: 8.3056e-04 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0312 - acc: 8.3056e-04 - val_loss: 0.0265 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0293 - acc: 8.3056e-04 - val_loss: 0.0226 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0272 - acc: 8.3056e-04 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0239 - acc: 8.3056e-04 - val_loss: 0.0177 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0213 - acc: 8.3056e-04 - val_loss: 0.0121 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0182 - acc: 8.3056e-04 - val_loss: 0.0103 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0160 - acc: 8.3056e-04 - val_loss: 0.0104 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0139 - acc: 8.3056e-04 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0119 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0117 - acc: 8.3056e-04 - val_loss: 0.0076 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0104 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0090 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0085 - acc: 8.3056e-04 - val_loss: 0.0072 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0087 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0089 - acc: 8.3056e-04 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0077 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0074 - acc: 8.3056e-04 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0069 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0070 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0055 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 9.0521e-04 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 9.5637e-04 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.7659e-04 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.7256e-04 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 9.9209e-04 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.5156e-04 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 9.3364e-04 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.5009e-04 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.5049e-04 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.9648e-04 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.9336e-04 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.2364e-04 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.7417e-04 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.7445e-04 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.8395e-04 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.8262e-04 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.4752e-04 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.8235e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.9168e-04 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.1616e-04 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.2628e-04 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.7588e-04 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.0186e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.6350e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.5037e-04 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.7915e-04 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.3746e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.7423e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.0298e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 6.7326e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.9214e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.6532e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.4202e-04 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 6.7969e-04 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.4310e-04 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.7337e-04 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.2259e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.7994e-04 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.9100e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.8062e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.1231e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.9525e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.5805e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.0974e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.2242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.7427e-04 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.2891e-04 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.6270e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.5745e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.9870e-04 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.4088e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6902e-04 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.0775e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.6448e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.5024e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.0835e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.1868e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.1852e-04 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.4209e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.8184e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.0812e-04 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.7996e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.3186e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.6355e-04 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.1273e-04 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.1381e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.9564e-04 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6023e-04 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.8680e-04 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.6299e-04 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.8580e-04 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.4807e-04 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.0156e-04 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6322e-04 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.6256e-04 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.1573e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.2578e-04 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.1445e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.5117e-04 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.2378e-04 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.5062e-04 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.0852e-04 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.3006e-04 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.9647e-04 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.7841e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.7532e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.5153e-04 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 5.9724e-04 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.1858e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.0500e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.8044e-04 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.0292e-04 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.8888e-04 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.1286e-04 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.8192e-04 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.9776e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.7144e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.2355e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 5.5905e-04 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.5370e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 5.8845e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.0396e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 5.4386e-04 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.3659e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00106 MSE (0.03 RMSE)\n",
      "Test Score: 0.00814 MSE (0.09 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_48 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_49 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_50 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 7s 6ms/step - loss: 0.2524 - acc: 8.3056e-04 - val_loss: 0.3543 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.1848 - acc: 8.3056e-04 - val_loss: 0.1107 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0436 - acc: 8.3056e-04 - val_loss: 0.0089 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0506 - acc: 0.0000e+00 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0382 - acc: 8.3056e-04 - val_loss: 0.0496 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0376 - acc: 8.3056e-04 - val_loss: 0.0656 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0400 - acc: 8.3056e-04 - val_loss: 0.0509 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0359 - acc: 8.3056e-04 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0347 - acc: 8.3056e-04 - val_loss: 0.0213 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0361 - acc: 8.3056e-04 - val_loss: 0.0274 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0347 - acc: 8.3056e-04 - val_loss: 0.0396 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0336 - acc: 0.0000e+00 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0325 - acc: 8.3056e-04 - val_loss: 0.0308 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0293 - acc: 8.3056e-04 - val_loss: 0.0237 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0264 - acc: 8.3056e-04 - val_loss: 0.0181 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0220 - acc: 8.3056e-04 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0185 - acc: 8.3056e-04 - val_loss: 0.0086 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0151 - acc: 8.3056e-04 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0123 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0103 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0095 - acc: 8.3056e-04 - val_loss: 0.0101 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0097 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0082 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0080 - acc: 8.3056e-04 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0073 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0073 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0064 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0066 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0057 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0065 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0057 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 343us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 330us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.8468e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 335us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 9.0765e-04 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.6583e-04 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.6780e-04 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.6650e-04 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.9473e-04 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.5427e-04 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.0505e-04 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.9662e-04 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.1397e-04 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.8071e-04 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.4439e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.5664e-04 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.1085e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.1409e-04 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.2141e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.3722e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.9681e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.5601e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.5837e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.2566e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 6.9461e-04 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.6928e-04 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.4302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.0249e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.5041e-04 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.4287e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.0905e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.4554e-04 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.7426e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.9474e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.1997e-04 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.9503e-04 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.7725e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 6.4701e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.4585e-04 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.5893e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.1608e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.8956e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.9241e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.2656e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.4331e-04 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.0205e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 7.0934e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.2678e-04 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.4494e-04 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.8615e-04 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.4016e-04 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.1407e-04 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.8154e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.6985e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.1343e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.0894e-04 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.0473e-04 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6786e-04 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.3843e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.9597e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.4844e-04 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.1881e-04 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.2820e-04 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.2934e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.7567e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.1414e-04 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.9751e-04 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.5167e-04 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.0290e-04 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.8377e-04 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.6889e-04 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.6274e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.8237e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.7153e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.4248e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 7.3219e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.3610e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00132 MSE (0.04 RMSE)\n",
      "Test Score: 0.00855 MSE (0.09 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_51 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_52 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_53 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 8s 7ms/step - loss: 0.2541 - acc: 8.3056e-04 - val_loss: 0.3657 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.1987 - acc: 8.3056e-04 - val_loss: 0.1555 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0600 - acc: 8.3056e-04 - val_loss: 0.0146 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0484 - acc: 0.0000e+00 - val_loss: 0.0100 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0429 - acc: 0.0000e+00 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0373 - acc: 8.3056e-04 - val_loss: 0.0674 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0412 - acc: 8.3056e-04 - val_loss: 0.0586 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0362 - acc: 8.3056e-04 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0349 - acc: 0.0000e+00 - val_loss: 0.0218 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0378 - acc: 0.0000e+00 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0353 - acc: 0.0000e+00 - val_loss: 0.0364 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0344 - acc: 8.3056e-04 - val_loss: 0.0455 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0345 - acc: 8.3056e-04 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0334 - acc: 8.3056e-04 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0322 - acc: 8.3056e-04 - val_loss: 0.0249 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0307 - acc: 8.3056e-04 - val_loss: 0.0247 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0274 - acc: 8.3056e-04 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0232 - acc: 8.3056e-04 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0200 - acc: 8.3056e-04 - val_loss: 0.0143 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0166 - acc: 8.3056e-04 - val_loss: 0.0066 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0141 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0124 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0106 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0100 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0094 - acc: 8.3056e-04 - val_loss: 0.0163 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0103 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0086 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0083 - acc: 8.3056e-04 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0083 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0078 - acc: 8.3056e-04 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0074 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0070 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0056 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0061 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0062 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0060 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.2105e-04 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.5206e-04 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.0342e-04 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.6871e-04 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.4280e-04 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.7609e-04 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.5937e-04 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.8209e-04 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.8218e-04 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.6435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.2254e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.8542e-04 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.9104e-04 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.0885e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.0057e-04 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.6727e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.7457e-04 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.0572e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.9693e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.1198e-04 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.2909e-04 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.2286e-04 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.1169e-04 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 8.7754e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.3711e-04 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.8205e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.8119e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.4959e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.0319e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.7751e-04 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.1720e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.3880e-04 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.4301e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.2684e-04 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.0004e-04 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.7854e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.6401e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.8346e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.6808e-04 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 8.4084e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.4001e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.4593e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6722e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.0222e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.9578e-04 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.1820e-04 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.1922e-04 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.4970e-04 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.3965e-04 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.2938e-04 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 5.9571e-04 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.1037e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.4059e-04 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.3898e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.1627e-04 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 5.9342e-04 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.4920e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.1437e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.8966e-04 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.3043e-04 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.7409e-04 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.5208e-04 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.5714e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.8784e-04 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.7373e-04 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.9744e-04 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.3346e-04 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.8664e-04 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.1729e-04 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 6.2679e-04 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.2791e-04 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.8879e-04 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.2430e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.9448e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.0812e-04 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 5.6742e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.2917e-04 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 5.4634e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00094 MSE (0.03 RMSE)\n",
      "Test Score: 0.00455 MSE (0.07 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_54 (LSTM)               (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (None, 22, 256)           394240    \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 22, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 663,617\n",
      "Trainable params: 663,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/300\n",
      "1204/1204 [==============================] - 9s 7ms/step - loss: 0.2564 - acc: 8.3056e-04 - val_loss: 0.3844 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.2180 - acc: 8.3056e-04 - val_loss: 0.2275 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.1032 - acc: 8.3056e-04 - val_loss: 0.0720 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0390 - acc: 8.3056e-04 - val_loss: 0.0085 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0510 - acc: 0.0000e+00 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0388 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0367 - acc: 8.3056e-04 - val_loss: 0.0658 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0407 - acc: 8.3056e-04 - val_loss: 0.0569 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0368 - acc: 8.3056e-04 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0340 - acc: 8.3056e-04 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0369 - acc: 0.0000e+00 - val_loss: 0.0233 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0344 - acc: 8.3056e-04 - val_loss: 0.0332 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0335 - acc: 8.3056e-04 - val_loss: 0.0413 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0328 - acc: 8.3056e-04 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0311 - acc: 8.3056e-04 - val_loss: 0.0279 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0297 - acc: 8.3056e-04 - val_loss: 0.0240 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0280 - acc: 8.3056e-04 - val_loss: 0.0227 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0257 - acc: 8.3056e-04 - val_loss: 0.0175 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0237 - acc: 8.3056e-04 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0207 - acc: 8.3056e-04 - val_loss: 0.0117 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0179 - acc: 8.3056e-04 - val_loss: 0.0098 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0153 - acc: 8.3056e-04 - val_loss: 0.0060 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0134 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0122 - acc: 8.3056e-04 - val_loss: 0.0072 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0108 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0093 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0082 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0078 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0068 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0065 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0055 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0057 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0057 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1204/1204 [==============================] - 0s 333us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 9.6314e-04 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1204/1204 [==============================] - 0s 334us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.4095e-04 - val_acc: 0.0000e+00\n",
      "Epoch 105/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 106/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.0803e-04 - val_acc: 0.0000e+00\n",
      "Epoch 107/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 108/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 9.9052e-04 - val_acc: 0.0000e+00\n",
      "Epoch 109/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 110/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 111/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 112/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 9.8324e-04 - val_acc: 0.0000e+00\n",
      "Epoch 113/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.8636e-04 - val_acc: 0.0000e+00\n",
      "Epoch 114/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.2707e-04 - val_acc: 0.0000e+00\n",
      "Epoch 115/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 116/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.4656e-04 - val_acc: 0.0000e+00\n",
      "Epoch 117/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.5537e-04 - val_acc: 0.0000e+00\n",
      "Epoch 118/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 119/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.3067e-04 - val_acc: 0.0000e+00\n",
      "Epoch 120/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 121/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 122/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 123/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.2403e-04 - val_acc: 0.0000e+00\n",
      "Epoch 124/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 7.8611e-04 - val_acc: 0.0000e+00\n",
      "Epoch 125/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 126/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.6622e-04 - val_acc: 0.0000e+00\n",
      "Epoch 127/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 128/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.6886e-04 - val_acc: 0.0000e+00\n",
      "Epoch 129/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 130/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 9.7435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 131/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 132/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.7970e-04 - val_acc: 0.0000e+00\n",
      "Epoch 133/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.9517e-04 - val_acc: 0.0000e+00\n",
      "Epoch 134/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 135/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 136/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 137/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 9.7923e-04 - val_acc: 0.0000e+00\n",
      "Epoch 138/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.9490e-04 - val_acc: 0.0000e+00\n",
      "Epoch 139/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 140/300\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 7.7537e-04 - val_acc: 0.0000e+00\n",
      "Epoch 141/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 142/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 8.7074e-04 - val_acc: 0.0000e+00\n",
      "Epoch 143/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 144/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 9.0090e-04 - val_acc: 0.0000e+00\n",
      "Epoch 145/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 9.9582e-04 - val_acc: 0.0000e+00\n",
      "Epoch 146/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 147/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 148/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 149/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 150/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 151/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 152/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 153/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 154/300\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 155/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 156/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 157/300\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.1327e-04 - val_acc: 0.0000e+00\n",
      "Epoch 158/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 159/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.2206e-04 - val_acc: 0.0000e+00\n",
      "Epoch 160/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 161/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 7.6837e-04 - val_acc: 0.0000e+00\n",
      "Epoch 162/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.4408e-04 - val_acc: 0.0000e+00\n",
      "Epoch 163/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.9971e-04 - val_acc: 0.0000e+00\n",
      "Epoch 164/300\n",
      "1204/1204 [==============================] - 0s 306us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 6.9934e-04 - val_acc: 0.0000e+00\n",
      "Epoch 165/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 166/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 8.4161e-04 - val_acc: 0.0000e+00\n",
      "Epoch 167/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 168/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 169/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 170/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 171/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 172/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 173/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 174/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.8812e-04 - val_acc: 0.0000e+00\n",
      "Epoch 175/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 176/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 177/300\n",
      "1204/1204 [==============================] - 0s 299us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 178/300\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.8597e-04 - val_acc: 0.0000e+00\n",
      "Epoch 179/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 180/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.0376e-04 - val_acc: 0.0000e+00\n",
      "Epoch 181/300\n",
      "1204/1204 [==============================] - 0s 338us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.0617e-04 - val_acc: 0.0000e+00\n",
      "Epoch 182/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.7314e-04 - val_acc: 0.0000e+00\n",
      "Epoch 183/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.9946e-04 - val_acc: 0.0000e+00\n",
      "Epoch 184/300\n",
      "1204/1204 [==============================] - 0s 344us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 8.4027e-04 - val_acc: 0.0000e+00\n",
      "Epoch 185/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.7578e-04 - val_acc: 0.0000e+00\n",
      "Epoch 186/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.5859e-04 - val_acc: 0.0000e+00\n",
      "Epoch 187/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 188/300\n",
      "1204/1204 [==============================] - 0s 324us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.2941e-04 - val_acc: 0.0000e+00\n",
      "Epoch 189/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 9.8428e-04 - val_acc: 0.0000e+00\n",
      "Epoch 190/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.7390e-04 - val_acc: 0.0000e+00\n",
      "Epoch 191/300\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.5038e-04 - val_acc: 0.0000e+00\n",
      "Epoch 192/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.9952e-04 - val_acc: 0.0000e+00\n",
      "Epoch 193/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 9.2587e-04 - val_acc: 0.0000e+00\n",
      "Epoch 194/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.9864e-04 - val_acc: 0.0000e+00\n",
      "Epoch 195/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 196/300\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 7.1196e-04 - val_acc: 0.0000e+00\n",
      "Epoch 197/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 198/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 199/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 200/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.8302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 201/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 9.9651e-04 - val_acc: 0.0000e+00\n",
      "Epoch 202/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 7.9954e-04 - val_acc: 0.0000e+00\n",
      "Epoch 203/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.4689e-04 - val_acc: 0.0000e+00\n",
      "Epoch 204/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.4302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 205/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.1901e-04 - val_acc: 0.0000e+00\n",
      "Epoch 206/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.0140e-04 - val_acc: 0.0000e+00\n",
      "Epoch 207/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.4895e-04 - val_acc: 0.0000e+00\n",
      "Epoch 208/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 6.4146e-04 - val_acc: 0.0000e+00\n",
      "Epoch 209/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.3237e-04 - val_acc: 0.0000e+00\n",
      "Epoch 210/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.1902e-04 - val_acc: 0.0000e+00\n",
      "Epoch 211/300\n",
      "1204/1204 [==============================] - 0s 313us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.1121e-04 - val_acc: 0.0000e+00\n",
      "Epoch 212/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.3085e-04 - val_acc: 0.0000e+00\n",
      "Epoch 213/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.5086e-04 - val_acc: 0.0000e+00\n",
      "Epoch 214/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6058e-04 - val_acc: 0.0000e+00\n",
      "Epoch 215/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 216/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.5563e-04 - val_acc: 0.0000e+00\n",
      "Epoch 217/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 218/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 9.1732e-04 - val_acc: 0.0000e+00\n",
      "Epoch 219/300\n",
      "1204/1204 [==============================] - 0s 344us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 220/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.6568e-04 - val_acc: 0.0000e+00\n",
      "Epoch 221/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 222/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.7148e-04 - val_acc: 0.0000e+00\n",
      "Epoch 223/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 224/300\n",
      "1204/1204 [==============================] - 0s 339us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.1420e-04 - val_acc: 0.0000e+00\n",
      "Epoch 225/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 226/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.1943e-04 - val_acc: 0.0000e+00\n",
      "Epoch 227/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.4199e-04 - val_acc: 0.0000e+00\n",
      "Epoch 228/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.3429e-04 - val_acc: 0.0000e+00\n",
      "Epoch 229/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 230/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.5101e-04 - val_acc: 0.0000e+00\n",
      "Epoch 231/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.5565e-04 - val_acc: 0.0000e+00\n",
      "Epoch 232/300\n",
      "1204/1204 [==============================] - 0s 331us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 6.6907e-04 - val_acc: 0.0000e+00\n",
      "Epoch 233/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.9990e-04 - val_acc: 0.0000e+00\n",
      "Epoch 234/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.2200e-04 - val_acc: 0.0000e+00\n",
      "Epoch 235/300\n",
      "1204/1204 [==============================] - 0s 325us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.6492e-04 - val_acc: 0.0000e+00\n",
      "Epoch 236/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.6185e-04 - val_acc: 0.0000e+00\n",
      "Epoch 237/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.4408e-04 - val_acc: 0.0000e+00\n",
      "Epoch 238/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.8138e-04 - val_acc: 0.0000e+00\n",
      "Epoch 239/300\n",
      "1204/1204 [==============================] - 0s 323us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.2911e-04 - val_acc: 0.0000e+00\n",
      "Epoch 240/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 6.9985e-04 - val_acc: 0.0000e+00\n",
      "Epoch 241/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.1947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 242/300\n",
      "1204/1204 [==============================] - 0s 301us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.4894e-04 - val_acc: 0.0000e+00\n",
      "Epoch 243/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 244/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.9124e-04 - val_acc: 0.0000e+00\n",
      "Epoch 245/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 246/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.4766e-04 - val_acc: 0.0000e+00\n",
      "Epoch 247/300\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.5873e-04 - val_acc: 0.0000e+00\n",
      "Epoch 248/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.0823e-04 - val_acc: 0.0000e+00\n",
      "Epoch 249/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.1675e-04 - val_acc: 0.0000e+00\n",
      "Epoch 250/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 8.2223e-04 - val_acc: 0.0000e+00\n",
      "Epoch 251/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.8545e-04 - val_acc: 0.0000e+00\n",
      "Epoch 252/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 253/300\n",
      "1204/1204 [==============================] - 0s 328us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.7052e-04 - val_acc: 0.0000e+00\n",
      "Epoch 254/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 8.0149e-04 - val_acc: 0.0000e+00\n",
      "Epoch 255/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.1699e-04 - val_acc: 0.0000e+00\n",
      "Epoch 256/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.0016e-04 - val_acc: 0.0000e+00\n",
      "Epoch 257/300\n",
      "1204/1204 [==============================] - 0s 329us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 258/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.5478e-04 - val_acc: 0.0000e+00\n",
      "Epoch 259/300\n",
      "1204/1204 [==============================] - 0s 326us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 260/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 7.0709e-04 - val_acc: 0.0000e+00\n",
      "Epoch 261/300\n",
      "1204/1204 [==============================] - 0s 322us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 8.2619e-04 - val_acc: 0.0000e+00\n",
      "Epoch 262/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.0889e-04 - val_acc: 0.0000e+00\n",
      "Epoch 263/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.4205e-04 - val_acc: 0.0000e+00\n",
      "Epoch 264/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 8.3655e-04 - val_acc: 0.0000e+00\n",
      "Epoch 265/300\n",
      "1204/1204 [==============================] - 0s 332us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.4170e-04 - val_acc: 0.0000e+00\n",
      "Epoch 266/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.2967e-04 - val_acc: 0.0000e+00\n",
      "Epoch 267/300\n",
      "1204/1204 [==============================] - 0s 341us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.5404e-04 - val_acc: 0.0000e+00\n",
      "Epoch 268/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.0895e-04 - val_acc: 0.0000e+00\n",
      "Epoch 269/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 7.8075e-04 - val_acc: 0.0000e+00\n",
      "Epoch 270/300\n",
      "1204/1204 [==============================] - 0s 336us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.1346e-04 - val_acc: 0.0000e+00\n",
      "Epoch 271/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 7.7798e-04 - val_acc: 0.0000e+00\n",
      "Epoch 272/300\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.5992e-04 - val_acc: 0.0000e+00\n",
      "Epoch 273/300\n",
      "1204/1204 [==============================] - 0s 335us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.1925e-04 - val_acc: 0.0000e+00\n",
      "Epoch 274/300\n",
      "1204/1204 [==============================] - 0s 312us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.8524e-04 - val_acc: 0.0000e+00\n",
      "Epoch 275/300\n",
      "1204/1204 [==============================] - 0s 317us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.7882e-04 - val_acc: 0.0000e+00\n",
      "Epoch 276/300\n",
      "1204/1204 [==============================] - 0s 321us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.5679e-04 - val_acc: 0.0000e+00\n",
      "Epoch 277/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.9555e-04 - val_acc: 0.0000e+00\n",
      "Epoch 278/300\n",
      "1204/1204 [==============================] - 0s 316us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 279/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.7291e-04 - val_acc: 0.0000e+00\n",
      "Epoch 280/300\n",
      "1204/1204 [==============================] - 0s 315us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 7.2368e-04 - val_acc: 0.0000e+00\n",
      "Epoch 281/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.7388e-04 - val_acc: 0.0000e+00\n",
      "Epoch 282/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.6370e-04 - val_acc: 0.0000e+00\n",
      "Epoch 283/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.4864e-04 - val_acc: 0.0000e+00\n",
      "Epoch 284/300\n",
      "1204/1204 [==============================] - 0s 310us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.7844e-04 - val_acc: 0.0000e+00\n",
      "Epoch 285/300\n",
      "1204/1204 [==============================] - 0s 308us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 6.0568e-04 - val_acc: 0.0000e+00\n",
      "Epoch 286/300\n",
      "1204/1204 [==============================] - 0s 303us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.2029e-04 - val_acc: 0.0000e+00\n",
      "Epoch 287/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.0035e-04 - val_acc: 0.0000e+00\n",
      "Epoch 288/300\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 6.2127e-04 - val_acc: 0.0000e+00\n",
      "Epoch 289/300\n",
      "1204/1204 [==============================] - 0s 307us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.5961e-04 - val_acc: 0.0000e+00\n",
      "Epoch 290/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 6.9345e-04 - val_acc: 0.0000e+00\n",
      "Epoch 291/300\n",
      "1204/1204 [==============================] - 0s 302us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.2686e-04 - val_acc: 0.0000e+00\n",
      "Epoch 292/300\n",
      "1204/1204 [==============================] - 0s 311us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 293/300\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 5.6525e-04 - val_acc: 0.0000e+00\n",
      "Epoch 294/300\n",
      "1204/1204 [==============================] - 0s 304us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.8323e-04 - val_acc: 0.0000e+00\n",
      "Epoch 295/300\n",
      "1204/1204 [==============================] - 0s 327us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 5.7026e-04 - val_acc: 0.0000e+00\n",
      "Epoch 296/300\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 9.5627e-04 - val_acc: 0.0000e+00\n",
      "Epoch 297/300\n",
      "1204/1204 [==============================] - 0s 319us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.8109e-04 - val_acc: 0.0000e+00\n",
      "Epoch 298/300\n",
      "1204/1204 [==============================] - 0s 318us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 299/300\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 6.3464e-04 - val_acc: 0.0000e+00\n",
      "Epoch 300/300\n",
      "1204/1204 [==============================] - 0s 314us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 7.2194e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00088 MSE (0.03 RMSE)\n",
      "Test Score: 0.00506 MSE (0.07 RMSE)\n"
     ]
    }
   ],
   "source": [
    "dlist = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "neurons_LSTM = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "dropout_result = {}\n",
    "\n",
    "for d in dlist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs)\n",
    "    dropout_result[rate] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "e54e04810025ef2a0644d1ef5ca7a7ef5d500008",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "D5SFEpepBEA6",
    "outputId": "d4ed8968-7dba-45e0-ee25-dc4a9f71b4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.4: 0.005058582065839495}\n",
      "[0.4]\n"
     ]
    }
   ],
   "source": [
    "min_val = min(dropout_result.values())\n",
    "min_val_key = [k for k, v in dropout_result.items() if v == min_val]\n",
    "print (dropout_result)\n",
    "print (min_val_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c37a710b6fd1b63a0939956afc50e134364c94ed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "9X40Ur7EBEBC",
    "outputId": "08e41a2e-7f98-435f-e4d0-2e12b76e5980"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(dropout_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Dropout')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abf33874a37d20c504d0fe3b18b45f80638a248a",
    "colab_type": "text",
    "id": "_eNbeP8XBEBG"
   },
   "source": [
    "12.2 Optimial epochs value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "c23ff7e4a241f877f4c7ca11e4d3a2046e3ef3de",
    "colab": {},
    "colab_type": "code",
    "id": "JenTTfekBEBG"
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128,256, 128, 32, 1]\n",
    "epochslist = [10,20,30,40,50,60,70,80,90,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "333cc6dd6f08447838c70d65df2329fdf3225a34",
    "colab": {},
    "colab_type": "code",
    "id": "j-xh_L3TBEBI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs_result = {}\n",
    "\n",
    "for epochs in epochslist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs)\n",
    "    epochs_result[epochs] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "4ffc9f2afbec2acd9057efe9e636884ff8ee6664",
    "colab": {},
    "colab_type": "code",
    "id": "W9dPvk_1BEBN"
   },
   "outputs": [],
   "source": [
    "lists = sorted(epochs_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eaa59d193f348e77665710ac201e3c0df0a2dac8",
    "colab_type": "text",
    "id": "AbFlK1FeBEBP"
   },
   "source": [
    "12.3 Optimal number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "96524a79d347301e771e09da192ac7fc2aa18c33",
    "colab": {},
    "colab_type": "code",
    "id": "x3jVc7OzBEBQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "epochs = 90\n",
    "dropout = 0.3\n",
    "neuronlist1 = [32, 64, 128, 256, 512]\n",
    "neuronlist2 = [16, 32, 64]\n",
    "neurons_result = {}\n",
    "\n",
    "for neuron_lstm in neuronlist1:\n",
    "    neurons = [neuron_lstm, neuron_lstm]\n",
    "    for activation in neuronlist2:\n",
    "        neurons.append(activation)\n",
    "        neurons.append(1)\n",
    "        trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs)\n",
    "        neurons_result[str(neurons)] = testScore\n",
    "        neurons = neurons[:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "09aefcc52568d09f14463ebf68c8f25aec3873d1",
    "colab": {},
    "colab_type": "code",
    "id": "-3lMfOKTBEBT"
   },
   "outputs": [],
   "source": [
    "lists = sorted(neurons_result.items())\n",
    "x,y = zip(*lists)\n",
    "\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('neurons')\n",
    "plt.ylabel('Mean Square Error')\n",
    "\n",
    "plt.bar(range(len(lists)), y, align='center')\n",
    "plt.xticks(range(len(lists)), x)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d54851b21fddb03e4eea327309fafb4a6294f149",
    "colab_type": "text",
    "collapsed": true,
    "id": "q67KoW6bBEBY"
   },
   "source": [
    "12.4 Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "fdbbc08ac45e7d9d6cb2cba4c0dc4a8e0bf14daa",
    "colab": {},
    "colab_type": "code",
    "id": "Yb0UKFQtBEBY"
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [512, 512, 64, 1]\n",
    "epochs = 90\n",
    "decaylist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d7683d11beca588e940cce056458542f397ef01f",
    "colab": {},
    "colab_type": "code",
    "id": "efWEAfoYBEBa"
   },
   "outputs": [],
   "source": [
    "def build_model3(layers, neurons, rate, decay):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(rate))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(rate))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "256133c12fd1a1ff7b17ef358292751fb485fd8d",
    "colab": {},
    "colab_type": "code",
    "id": "wYMUOBFIBEBf"
   },
   "outputs": [],
   "source": [
    "def quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model3(shape, neurons, rate, decay)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "9d2dee6020174528833f0b6b7e693d5b9e094907",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 31229
    },
    "colab_type": "code",
    "id": "w83DyDrUBEBh",
    "outputId": "665d2a8f-5fff-4176-fce3-f5c988cce5f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 2s 2ms/step - loss: 0.2307 - acc: 8.3056e-04 - val_loss: 0.0129 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.1996 - acc: 8.3056e-04 - val_loss: 0.0489 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0600 - acc: 8.3056e-04 - val_loss: 0.1628 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 267us/step - loss: 0.0796 - acc: 8.3056e-04 - val_loss: 0.0685 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 268us/step - loss: 0.0284 - acc: 8.3056e-04 - val_loss: 0.0136 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0345 - acc: 8.3056e-04 - val_loss: 0.0110 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 267us/step - loss: 0.0354 - acc: 8.3056e-04 - val_loss: 0.0269 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 269us/step - loss: 0.0300 - acc: 8.3056e-04 - val_loss: 0.0458 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 269us/step - loss: 0.0303 - acc: 8.3056e-04 - val_loss: 0.0452 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 266us/step - loss: 0.0275 - acc: 8.3056e-04 - val_loss: 0.0246 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 268us/step - loss: 0.0225 - acc: 8.3056e-04 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0195 - acc: 8.3056e-04 - val_loss: 0.0148 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 266us/step - loss: 0.0135 - acc: 8.3056e-04 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0071 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0064 - acc: 8.3056e-04 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0060 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 268us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 268us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 269us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0050 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0050 - acc: 8.3056e-04 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 268us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 269us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 269us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00165 MSE (0.04 RMSE)\n",
      "Test Score: 0.00273 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 2s 2ms/step - loss: 0.2205 - acc: 8.3056e-04 - val_loss: 0.0063 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0166 - acc: 8.3056e-04 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0137 - acc: 8.3056e-04 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0116 - acc: 8.3056e-04 - val_loss: 0.0263 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0109 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0081 - acc: 8.3056e-04 - val_loss: 0.0135 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0068 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0066 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 9.7158e-04 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.6560e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.3679e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 8.8976e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.1243e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.7750e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.9262e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.5969e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.7117e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.5105e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.5051e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.9978e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.4576e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.6836e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.4255e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.8333e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.5255e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 9.1588e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 6.7891e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 6.6140e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00104 MSE (0.03 RMSE)\n",
      "Test Score: 0.00136 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 3s 2ms/step - loss: 0.2398 - acc: 8.3056e-04 - val_loss: 0.0871 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0779 - acc: 8.3056e-04 - val_loss: 0.0180 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0268 - acc: 8.3056e-04 - val_loss: 0.0902 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0403 - acc: 8.3056e-04 - val_loss: 0.0381 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0239 - acc: 8.3056e-04 - val_loss: 0.0106 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0301 - acc: 8.3056e-04 - val_loss: 0.0147 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0208 - acc: 8.3056e-04 - val_loss: 0.0351 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0199 - acc: 8.3056e-04 - val_loss: 0.0314 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0131 - acc: 8.3056e-04 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0110 - acc: 8.3056e-04 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0078 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0067 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.7302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.9559e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.9199e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 8.9857e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.4648e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.8954e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.9502e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 8.4118e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00137 MSE (0.04 RMSE)\n",
      "Test Score: 0.00154 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 3s 3ms/step - loss: 0.2231 - acc: 8.3056e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0185 - acc: 8.3056e-04 - val_loss: 0.0579 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0211 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0165 - acc: 8.3056e-04 - val_loss: 0.0230 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0135 - acc: 8.3056e-04 - val_loss: 0.0119 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0091 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0070 - acc: 8.3056e-04 - val_loss: 0.0128 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.6730e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.4158e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 9.4799e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.4236e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.5208e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.6638e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.2264e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.1168e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.0883e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.7591e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.7114e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.2031e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.8311e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.6943e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.1821e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 8.1018e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.7579e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 9.7849e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.2803e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.0729e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 6.9965e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00149 MSE (0.04 RMSE)\n",
      "Test Score: 0.00361 MSE (0.06 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 3s 3ms/step - loss: 0.2206 - acc: 8.3056e-04 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0202 - acc: 8.3056e-04 - val_loss: 0.0490 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0182 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0148 - acc: 8.3056e-04 - val_loss: 0.0275 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0140 - acc: 8.3056e-04 - val_loss: 0.0067 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0088 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0064 - acc: 8.3056e-04 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.9273e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.4737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.4469e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 8.9309e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.0880e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 9.6929e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.3498e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.0960e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.8229e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.0939e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.9417e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.4306e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 7.4972e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.2087e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.5965e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.0277e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.5086e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 6.9063e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.9574e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00115 MSE (0.03 RMSE)\n",
      "Test Score: 0.00248 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 4s 3ms/step - loss: 0.2288 - acc: 8.3056e-04 - val_loss: 0.0112 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.1687 - acc: 8.3056e-04 - val_loss: 0.0426 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0533 - acc: 8.3056e-04 - val_loss: 0.1418 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0638 - acc: 8.3056e-04 - val_loss: 0.0520 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0264 - acc: 8.3056e-04 - val_loss: 0.0128 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0352 - acc: 8.3056e-04 - val_loss: 0.0151 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0325 - acc: 8.3056e-04 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0293 - acc: 8.3056e-04 - val_loss: 0.0479 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0302 - acc: 8.3056e-04 - val_loss: 0.0411 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0263 - acc: 8.3056e-04 - val_loss: 0.0200 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0209 - acc: 8.3056e-04 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0150 - acc: 8.3056e-04 - val_loss: 0.0172 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0093 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0074 - acc: 8.3056e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0075 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0052 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0056 - acc: 8.3056e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0049 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0040 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0063 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 320us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 1s 480us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 309us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00150 MSE (0.04 RMSE)\n",
      "Test Score: 0.00241 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 4s 3ms/step - loss: 0.2216 - acc: 8.3056e-04 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0192 - acc: 8.3056e-04 - val_loss: 0.0249 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0118 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0102 - acc: 8.3056e-04 - val_loss: 0.0185 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0085 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0070 - acc: 8.3056e-04 - val_loss: 0.0122 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0063 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.2987e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.5861e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.1496e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.4726e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 8.8296e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.8199e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.0265e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.2577e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.3571e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.6250e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.3823e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.2906e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.7238e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 7.0825e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.5848e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.2287e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 6.8251e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 6.8998e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00122 MSE (0.03 RMSE)\n",
      "Test Score: 0.00263 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_20 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_21 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 5s 4ms/step - loss: 0.2305 - acc: 8.3056e-04 - val_loss: 0.0086 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.2066 - acc: 8.3056e-04 - val_loss: 0.0754 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0759 - acc: 8.3056e-04 - val_loss: 0.1851 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0932 - acc: 8.3056e-04 - val_loss: 0.0783 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0311 - acc: 8.3056e-04 - val_loss: 0.0129 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0336 - acc: 8.3056e-04 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0348 - acc: 8.3056e-04 - val_loss: 0.0276 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0296 - acc: 8.3056e-04 - val_loss: 0.0497 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0317 - acc: 8.3056e-04 - val_loss: 0.0478 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0283 - acc: 8.3056e-04 - val_loss: 0.0263 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0232 - acc: 8.3056e-04 - val_loss: 0.0097 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0197 - acc: 8.3056e-04 - val_loss: 0.0127 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0131 - acc: 8.3056e-04 - val_loss: 0.0081 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0077 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0059 - acc: 8.3056e-04 - val_loss: 0.0067 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0084 - acc: 8.3056e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0064 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0052 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0053 - acc: 8.3056e-04 - val_loss: 0.0049 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0044 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0041 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0065 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0045 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 305us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00216 MSE (0.05 RMSE)\n",
      "Test Score: 0.00189 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 5s 4ms/step - loss: 0.2188 - acc: 8.3056e-04 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0194 - acc: 8.3056e-04 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0104 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0097 - acc: 8.3056e-04 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0085 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0066 - acc: 8.3056e-04 - val_loss: 0.0107 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0054 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0048 - acc: 8.3056e-04 - val_loss: 0.0068 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0044 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0041 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0037 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 9.5226e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.5617e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 9.0186e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.5415e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 8.4887e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.0588e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.7434e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.2194e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 9.8552e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.9691e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.6265e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.1026e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.8646e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 7.9804e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.0996e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00110 MSE (0.03 RMSE)\n",
      "Test Score: 0.00148 MSE (0.04 RMSE)\n"
     ]
    }
   ],
   "source": [
    "decay_result = {}\n",
    "\n",
    "for decay in decaylist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs, decay)\n",
    "    decay_result[decay] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "556573a23bdaa8ef373851de39269bd927288c4a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "F7fMCAFZBEBj",
    "outputId": "54155e92-e1b8-4396-9415-46dac585fad4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8VfX9+PHXOztkASEDkkDYkCBD\nIqLiqApx1KKtINpabW3V/tTub1trh9qlbdVvW+2wtXV9q6K1LVorqKBWRSBIAoQZZhL2yABC5vv3\nxzmh15hxA7n7/Xw8zoNzz/2cc9+5JPd9z2eKqmKMMcb0tahAB2CMMSY8WYIxxhjjE5ZgjDHG+IQl\nGGOMMT5hCcYYY4xPWIIxxhjjE5ZgTI9EZKiIHBGR6JM8f7uIXOzuf1dE/tS3EXb5uheISFUfXetG\nEXmnL64VCq9rTF+wBGNOcBNBg5tM2rchqrpTVZNVtfVUX0NVf6qqX+iLeDsSERWRUb64tq+EYsyh\nyPNLjvEfSzCmoyvcZNK+7Qp0QCbwTvbutZvrxfTl9XxJHPZZeRLsTTM9EpF895t2jPv4TRH5kYi8\nKyL1IrJIRAZ5lL9eRHaIyEERuavDte4Wkac7XPcGEdkpIgc8y4tIoog8ISKHRWS9iHyrqyovEXnb\n3S1z77yu8XjuGyKyT0R2i8jnPI7Hi8gv3dfeKyK/F5HE7t8KeVhEakVkg4hc5PFEmog85r5GtYj8\nuP1DWURGichb7nkHROS5nmLu5IV/6b4P20TkUvfYHBFZ2aHc10Xkn+7+4+7P9Jr7//SWiAzzKDvO\nfe6QiGwUkbkezz0uIr8TkVdE5CjwMS+u9ysRqRSROhFZKSLnejx3t4i8ICJPi0gdcKOITBORpSJS\n475vD4tInMc5KiL/T0Q2u6/3IxEZKSLvua8xv0P5j4tIqXu990Rkonv8KWAo8JL7Pn/LPT7dLVcj\nImUicoHHtd4UkZ+IyLvAMWBE178WpkuqapttqCrAduDiTo7nAwrEuI/fBLYAY4BE9/F97nMFwBHg\nPCAeeBBoab8ucDfwdIfr/tG9ziSgERjvPn8f8BYwAMgFVgNV3cSvwCiPxxe4r30vEAtchvNhMcB9\n/iFgATAQSAFeAn7WxbVvdK/1Nfda1wC1wED3+b8DfwCSgExgOXCL+9wzwF04X+gSgBldxdzF6zYD\nXwSigS8BuwBx399D7e+XW34V8Cl3/3Gg3uP/4lfAO+5zSUAl8DkgBpgCHAAKPM6tBc7xiLvL67nn\nfAZId6/3DWAPkODx/94MXOleLxGYCkx3y+cD64Gvdnhv/gmkAoXu78YbOB/2acA64Aa37BRgH3Cm\n+z7dgPP7HN/Z7zaQAxx0fyeigJnu4wyP3/Gd7uvGALGB/vsMxS3gAdgWPJv7R3gEqHG3f7jH8/lo\ngvmex3n/D3jV3f8B8KzHc0lAE90nmFyP8suBee7+VqDY47kv0PsE09Aet3tsn/uhJsBRYKTHc2cB\n27q49o24H+wdYr0eyHI//BI9nrsWWOLuPwk86vlzdhVzF69b4fG4n3tOtvv4d8BP3P1C4LDHh+rj\nHf4vkoFWIA8nQf6nw2v9Afihx7lPdni+y+t1EfthYJLH//vbPfz+fRX4e4f35hyPxyuBb3s8fgD4\nX4/34UcdrrcRON/jd9szwXwbeKpD+YX8N2G9CdwbyL/HcNisisx0dKWq9ne3K7spt8dj/xjOhw3A\nEJxvxgCo6lGcb4bd8epaHfa9dVBVWzq5fgbOh/VKt4qkBnjVPd6VanU/fVw73BiH4dzV7Pa41h9w\n7mQAvoWT0JaLSLmIfL6XP8OJ90dVj7m77e/RE8B1IiI4yW6+qjZ6nOv5f3EE546nPeYz2+N1Y/40\nkN3ZuV5cDxH5pluVWeteLw0Y1Nm5bvkxIvKyiOxxq81+2qE8wF6P/YZOHre/D8OAb3T4efLaY+vE\nMGBOh/IzgME9/PymF0Kmoc2EjN3A+PYHItIPp9rkZK+Vi1MVAs4HRl85gPMBVaiq1V6ekyMi4pFk\nhuJUsVXi3MEM6pDMAFDVPThVXIjIDOB1EXlbVStO9YdQ1fdFpAk4F7jO3TydeM9EJBmnOnCXG/Nb\nqjqzu8t3cqzT67ntLd8CLgLKVbVNRA7jJNaurvc7nCq9a1W1XkS+ClzdTTzdqcS5k/tJF893fO1K\nnDuYL3ZzTZtq/hTZHYzpay8AHxeRGW4D7L2c/O/ZfOBOERkgIjnA7T2U34uXjbGq2obT9vOQiGQC\niEiOiBR3c1om8GURiRWROTiJ9BVV3Q0sAh4QkVQRiXIbo893rztHRHLdaxzG+eBq623M3XgSeBho\nVtWOY2Yu8/i/+BHwvqpWAi8DY8TpkBHrbmeIyHi619X1UnDaqPYDMSLyA5y2k+6kAHXAEREZh9O+\ndLL+CNwqImeKI0lELheRFPf5ju/z08AVIlIsItEikiDOuKncj1zZnDRLMKZPqWo5cBvwV5w7kMPA\nyQ52vNc9dxvwOk7yauym/N3AE26Vx9xuyrX7NlABvO9W0bwOjO2m/DJgNM7dz0+Aq1W1vfrvs0Ac\nzt3WYTfW9uqWM4BlInIE547nK6q69SRj7sxTwAScD82O/gr8EKcqaypOQzyqWg/MAubh3NHsAe7H\nabzvTqfXw2m/eBXYhFN1eJyeq5i+iXPHVY+TIJ7roXyXVLUE5y7xYZz3vwKn/ardz4Dvue/zN92k\nOBv4Lk5SrAT+B/tM7FPy4SplY4KXiHwJpwPA+YGOJZiI07V6H3C6qm72OP44TqeI7/XR6/Tp9Uz4\ns2xtgpaIDBaRc9wqp7E4XV//Hui4gtCXgBWeycWYYGCN/CaYxeH0xhqO0236WeC3AY0oyIjIdpyG\n9O56/BkTEFZFZowxxiesiswYY4xPRHQV2aBBgzQ/Pz/QYRhjTEhZuXLlAVXtblAyEOEJJj8/n5KS\nkkCHYYwxIUVEdnhTzqrIjDHG+IQlGGOMMT5hCcYYY4xPWIIxxhjjE5ZgjDHG+IQlGGOMMT5hCcYY\nY4xPWIIxEWt/fSP/WFWNTZdkjG9YgjER64n3tvPV50pZU10b6FCMCUuWYEzEKquqAWB+iS29bowv\nWIIxEUlVKat0Esw/S3dxvLk1wBEZE34swZiItP3gMeqOt3DVlBzqj7fw6to9gQ7JmLBjCcZEpPa7\nly+eO4KhA/vx3AqrJjOmr1mCMRGptLKGxNhoxmQlM2dqLku3HmTnwWOBDsuYsGIJxkSk1VU1nJaT\nRkx0FFcX5SICL6y0uxhj+pJPE4yIXCIiG0WkQkS+08nz8SLynPv8MhHJ93juTvf4RhEpdo8liMhy\nESkTkXIRucej/OMisk1ESt1tsi9/NhO6mlvbKN9Vx6S8NAAGpyVy3ugMnl9ZRWubjYkxpq/4LMGI\nSDTwCHApUABcKyIFHYrdBBxW1VHAQ8D97rkFwDygELgE+K17vUbgQlWdBEwGLhGR6R7X+x9Vnexu\npb762Uxo27innsaWNibm9j9xbG5RHrtrj/NOxYEARmZMePHlHcw0oEJVt6pqE/AsMLtDmdnAE+7+\nC8BFIiLu8WdVtVFVtwEVwDR1HHHLx7qbfeU0vdI+/mVy3n8TzMUFmQzoF2tjYozpQ75MMDmA519r\nlXus0zKq2gLUAundnSsi0SJSCuwDXlPVZR7lfiIiq0XkIRGJ7ywoEblZREpEpGT//v0n/9OZkFVW\nWcOAfrHkDkg8cSw+Jporp+TwWvleDh9tCmB0xoSPkGvkV9VWVZ0M5ALTRGSC+9SdwDjgDGAg8O0u\nzn9UVYtUtSgjI8MvMZvgsrqqlkl5/XFulv9rblEeTa1t/KO0OkCRGRNefJlgqoE8j8e57rFOy4hI\nDJAGHPTmXFWtAZbgtNGgqrvdKrRG4C84VXTGfMixphY27a1nkkf7S7vxg1OZmJvGcysqbQJMY/qA\nLxPMCmC0iAwXkTicRvsFHcosAG5w968GFqvzl70AmOf2MhsOjAaWi0iGiPQHEJFEYCawwX082P1X\ngCuBtT782UyIWltdR5tyogdZR3OK8tiwp5611XV+jsyY8OOzBOO2qdwOLATWA/NVtVxE7hWRT7jF\nHgPSRaQC+DrwHffccmA+sA54FbhNVVuBwcASEVmNk8BeU9WX3Wv9n4isAdYAg4Af++pnM6GrfQT/\nxE7uYAA+MWkI8TFR1thvTB+I8eXFVfUV4JUOx37gsX8cmNPFuT8BftLh2GpgShflLzzVeE34K62q\nIad/IoOSO+0DQlpiLJdOyOafpdXcdfl4EmKj/RyhMeEj5Br5jTkVq6tqPtQ9uTNzi/KoO97CwnKb\nANOYU2EJxkSMg0caqTzUwMTczttf2k0fkU7ewESrJjPmFFmCMRFjtbty5aQe7mCiooQ5U/N4t+Ig\nlYdsAkxjTpYlGBMxyipriBI4Laf7OxiAT011JsB8fmWVHyIzJjxZgjERo6yyhlGZySTF99y3Jad/\nIjNGDeKFkkqbANOYk2QJxkQEVXVG8HfRPbkz15yRx67a47y3xSbANOZkWIIxEaHqcAMHjzYxsYf2\nF08zC7Lo3y/WVrs05iRZgjERYXWV08A/uRd3MPEx0Vw5OYdF5XupOWYTYBrTW5ZgTEQoq6ohLiaK\nsdkpvTqvfQLMf5bu8lFkxoQvSzAmIpRW1lAwOJW4mN79yhcMSWVCTqpVkxlzEizBmLDX2qasra7t\ncQR/V+YW5bFudx1r3XE0xhjvWIIxYa9i3xGONbX2OIK/K7Mn5RAXE8XzNrLfmF6xBGPCXvsSyT2N\n4O9KWr9YLinM5h+luzje3NqXoRkT1izBmLBXVllDSkIMw9OTTvoac4vyqG1oZtG6vX0YmTHhzRKM\nCXtlVTVMzE0jKkp6LtyFs0emk9M/0arJjOkFSzAmrB1vbmXD7s6XSO6NqChhTlEu71QcoOqwTYBp\njDcswZiwtm53HS1t2uUKlr1x9dRcAF6wCTCN8YolGBPWVrtLJJ9sF2VPuQP6MWPUIJ4vqaLNJsA0\npkeWYExYK6uqJSs1nuy0hD653pyiPKprGnhvy8E+uZ4x4cwSjAlrZZU1fVI91m5WQRZpibG22qUx\nXrAEY8JWbUMzWw8c7ZPqsXYJsdFcOXkIr5bvofZYc59d15hwZAnGhK017gzKJzuCvytzivJoamnj\nn2XVfXpdY8KNJRgTttpH8E/M6bs7GIAJOWkUDE61ajJjemAJxoStssoaRgxKIq1fbJ9f+5oz8lhb\nXUf5LpsA05iuWIIxYat9BL8vzJ48hLjoKJ4vsTExxnTFEowJS3tqj7O3rvGkJ7jsSf9+ccwqzOLv\nq6ptAkxjumAJxoSlE+0vfdhFuaNrznAmwHx9vU2AaUxnLMGYsLS6qoaYKKFwSKrPXuPskYPI6Z9o\nq10a0wVLMCYslVXWMm5wCgmx0T57jego4VNTnQkwq2safPY6xoQqSzAm7LS1qdvA77vqsXZzpuai\nCn+zCTCN+QhLMCbsbD94lPrjLUz2Q4LJG9iPc0alM7+k0ibANKYDSzAm7Jxo4M/zTRfljuYW5VF1\nuIH3t9oEmMZ4sgRjwk5ZZS394qIZnZnil9crLswmNSHGRvYb04ElGBN2yqpqmJCTRvQpLJHcGwmx\n0cyenMO/1+6htsEmwDSmnU8TjIhcIiIbRaRCRL7TyfPxIvKc+/wyEcn3eO5O9/hGESl2jyWIyHIR\nKRORchG5x6P8cPcaFe4143z5s5ng1NTSRvmuOib5aAR/V+YW5dHY0saCsl1+fV1jgpnPEoyIRAOP\nAJcCBcC1IlLQodhNwGFVHQU8BNzvnlsAzAMKgUuA37rXawQuVNVJwGTgEhGZ7l7rfuAh91qH3Wub\nCLNpbz1NLW0+G8HflQk5qYwfnMp8GxNjzAm+vIOZBlSo6lZVbQKeBWZ3KDMbeMLdfwG4SETEPf6s\nqjaq6jagApimjiNu+Vh3U/ecC91r4F7zSl/9YCZ4lbpLJE/yQw8yTyLC3KJc1lTXsm5XnV9f25hg\n5csEkwN4fp2rco91WkZVW4BaIL27c0UkWkRKgX3Aa6q6zD2nxr1GV69lIsDqqhoGJsWROyDR7699\n5eQcZwLMlXYXYwz0kGDcD/Ov+SsYb6hqq6pOBnKBaSIyoTfni8jNIlIiIiX79+/3TZAmYMoqa5mY\nm4ZzU+tfA5LimOlOgNnYYhNgGtNtglHVVuDak7x2NZDn8TjXPdZpGRGJAdKAg96cq6o1wBKcNpqD\nQH/3Gl29Vvt5j6pqkaoWZWRknMSPZYLV0cYWNu+r93v1mKe5RXnUHGvm9XX7AhaDMcHCmyqyd0Xk\nYRE5V0ROb9+8OG8FMNrt3RWH02i/oEOZBcAN7v7VwGJVVff4PLeX2XBgNLBcRDJEpD+AiCQCM4EN\n7jlL3GvgXvOfXsRowsja6lraFCb7uYHf04xRgxiSlmBjYowBYnouwmT333s9jilOo3qXVLVFRG4H\nFgLRwJ9VtVxE7gVKVHUB8BjwlIhUAIdwkhBuufnAOqAFuE1VW0VkMPCE26MsCpivqi+7L/lt4FkR\n+TGwyr22iSD/naLfv12UPUVHCVdPzeU3SyrYVdPAkP7+bwsyJliI8+U/MhUVFWlJSUmgwzB95La/\nfkBZZQ3vfLvb7z4+t/PgMc77xRK+MXMMd1w0OqCxGOMLIrJSVYt6KtdjFZmIpInIg+0N4yLygIgE\n7iuiMV0oq6wJaPtLu6Hp/ThrRDrPr6yyCTBNRPOmDebPQD0w193qgL/4MihjeuvgkUaqDjcwyU8T\nXPbkmjPy2HnoGO9vswkwTeTyJsGMVNUfugMmt6rqPcAIXwdmTG+srqoF/D/AsiuXTMgmJSGG50ts\nnRgTubxJMA0iMqP9gYicA9jyfSaolFbWECUwISc47mASYqP5xKQhvLJmN3XHbQJME5m8STC3Ao+I\nyHYR2Q48DNzi06iM6aXVVTWMzkwhKd6bjpH+cc0Z7gSYpTYBpolMPY3kjwLGupNLTgQmquoUVV3t\nl+iM8YKqUlZVG9DuyZ05LSeNcdkpPG9jYkyE6mkkfxvwLXe/TlVtFj8TdKoON3DoaJPfZ1DuiTMB\nZh5lVbVs2GN/OibyeFNF9rqIfFNE8kRkYPvm88iM8VL7AMtAjuDvypVTcoiNFuavsMZ+E3m8STDX\nALcBbwMr3c1GJ5qgUVZZQ1xMFGOz/bNEcm8MTIpjZkEWf19VRVNLW6DDMcavvGmD+YyqDu+wWTdl\nEzTKqmopHJJKbHRwrgA+tyiPw8eaeWP93kCHYoxfedMG87CfYjGm11rblLXVtUEz/qUz547OIDs1\ngeessd9EGG++8r0hIp+SQCywYUwPKvYd4VhTa9CM4O9M+wSYb2/az+5aG0JmIoc3CeYW4HmgUUTq\nRKReRKxLjAkKZQFaIrm35hTl0qbw4gedLlNkTFjqMcGoaoqqRqlqnKqmuo9T/RGcMT0praohJSGG\n/PSkQIfSrWHpSUwfMZD5JZU2AaaJGF0mGBH5jMf+OR2eu92XQRnjrdVVzgzKUVHBX4M7tyiPHQeP\nsXz7oUCHYoxfdHcH83WP/d90eO7zPojFmF453tzKht31QTeCvyuXThhMSnyMrXZpIkZ3CUa62O/s\nsTF+t253HS1tGnQj+LuSGBfNFZNtAkwTObpLMNrFfmePI8riDXv5zRubAx1GxGtv4A/GEfxdmVuU\nx/HmNl4u2x3oUIzxue4SzDgRWS0iazz22x+P9VN8QWnZtkP86o3N1DbYt9BAKqusISs1nqzUhECH\n4rVJuWmMzUqxajITEbpLMOOBK4CPe+y3Py7wfWjBq7gwm5Y2ZcmGfYEOJaKtrgruAZadERHmFOVS\nWlnDpr31gQ7HGJ/qMsGo6o7uNn8GGWwm5/YnMyWeRev2BDqUiFXb0MzWA0dDpv3F01UnJsC0uxgT\n3oJz8qYgFxUlzCzI4s2N+zne3BrocCLSmiBbIrk30pPjuXh8Fn9fVW0TYJqwZgnmJBUXZnOsqZV3\nNh8IdCgRqX2K/tNCpItyR3OL8jh4tInFG2wCTBO+vEowIpIoIhHdsN/R9BHppCTEWDVZgJRW1jBi\nUBJpibGBDuWknDt6EFmp8cwvsXViTPjqMcGIyBVAKfCq+3iyiCzwdWDBLi4migvHZfL6+n20tFo1\nh7+trqoJyfaXdjHRUVw9NZc3N+5jb93xQIdjjE94cwdzNzANqAFQ1VJguA9jChnFhdkcOtpEyY7D\ngQ4louypPc7eusaQGcHflTlT82hTeGGl3cWY8ORNgmlW1doOxyJ6oGW788dkEBcTxcJyqybzp/b2\nl1C+gwHIH5TEtOEDeb6kElX7kzpZza1t1uU7SHmTYMpF5DogWkRGi8hvgPd8HFdISIqP4dxRg1hU\nvtc+IPyorLKGmCihYHDoT+p9TVEe2w8eY8V2uwvuraaWNp5ZvpOP/fJNZj30Nq+utS96wcabBHMH\nUAg0An8FaoGv+jKoUFJcmE11TQPlu2yJHH8pq6ph3OAUEmKjAx3KKbv0tGyS42N4zsbEeO14cytP\nLd3OBb9Ywp0vriE9OZ4Rg5L48b/W2bCBINNtghGRaOBeVb1LVc9wt++pqrVKui4an0mUwCKrJvOL\ntjYNyRH8XekXF8MVkwbzyprd1NsEmN063tzKX97dxvm/WML3/1nO4P6JPPn5afzj/53Nj6+cQNXh\nBv749tZAh2k8dJtgVLUVmOGnWEJSenI8RfkDWbTOxjP4w7aDR6k/3hI2CQacMTENza38a7VNgNmZ\nY00t/PHtrcy4fwn3vLSO/PQk/vqFM3nh1rM4b0wGIsLZowZx6YRsHnmzgl01tix1sPCmimyViCwQ\nketF5JPtm88jCyHFhdls2FPPjoNHAx1K2FsdJg38nibn9Wd0ZjLP2QSYH3KksYXfvbmFGfcv4Sev\nrGdcdgrP3Tyd5245i7NHDULkw6uGfPey8ajCT19ZH6CITUfeJJgE4CBwIR+e8NK4ZhVkAVhvMj8o\nq6ylX1w0ozKTAx1KnxER5hblsWpnDZutNxR1x5t5ePFmZty/mPtf3cBpOWn87Utn8fQXzuTMEeld\nnpc3sB+3nj+Sl1fvZtnWg36M2HQlpqcCqvo5fwQSyvIG9qNwSCoLy/dy83kjAx1OWCutrGFCThrR\nIbBEcm9cdXoO97+6gedXVvHdy8YHOpyAqD3WzF/e28af39lG3fEWLh6fye0Xju7Vej+3nj+S50sq\n+eGCcl6+YwYx0TYbViB5M5I/QURuE5Hfisif2zd/BBdKZhVk88HOw+yrt/4PvtLU0sa63XUhtcCY\ntwYlx3PR+Exe/KCK5gibGeLw0SZ+uXAjM+5fzP++vpmzRqbz8h0z+NMNZ/T6/zoxLpq7Li9gw556\nnrGeeQHnTXp/CsgGioG3gFzAq/t4EblERDaKSIWIfKeT5+NF5Dn3+WUiku/x3J3u8Y0iUuweyxOR\nJSKyTkTKReQrHuXvFpFqESl1t8u8ibGvFE/IQhVeX2drxPjKxj31NLW0hfwI/q7MLcrjwJEmFkfI\nOkMHjjRy3783MOP+xTzyZgXnjcng3185lz9cX8SEnJP/P77stGymjxjIA4s2UnOsqQ8jNr3lTYIZ\nparfB46q6hPA5cCZPZ3kdnF+BLgUZ4Gya0Wk40JlNwGHVXUU8BBwv3tuATAPZ/zNJcBv3eu1AN9Q\n1QJgOnBbh2s+pKqT3e0VL362PjM2K4Vh6f2sHcaHTozgD6MeZJ7OH5NBZkp82K8Ts6/+OD9+eR3n\n3r+ER9/ewkXjs1j41fN45NOnM74PBs+KCHd/opC6hmYefG1TH0RsTlaPbTBAe+f8GhGZAOwBMr04\nbxpQoapbAUTkWWA2sM6jzGycuc4AXgAeFqdryGzgWVVtBLaJSAUwTVWXArsBVLVeRNYDOR2uGRAi\nwqyCLB5/bzv1x5tJSQjNWX6DWVllDQOT4sgdkBjoUHwiJjqKT03N5Q9vbWFv3fGQWgraG3tqj/P7\nt7bwzPKdtLQpsycP4baPjWJkRt932BiXncpnpg/j6fd3cO20oX2SuEzveXMH86iIDAC+DyzA+TD/\nuRfn5QCeX8Wq3GOdllHVFpxZAtK9OdetTpsCLPM4fLuIrHbbiQZ4EWOfKi7MprlVWbJxv79fOiKU\nVdUwKTftI91Tw8mcqbm0Kbz4QXWgQ+kz1TUNfP8faznv50t4+v0dzJ48hDe+fj4Pzp3sk+TS7usz\nx5CaGMs9L5XbVE4B0mOCUdU/qephVX1LVUeoaqaq/t4fwXVFRJKBvwFfVdX2OVp+B4wEJuPc5TzQ\nxbk3i0iJiJTs39+3iWDK0AEMSo63ajIfONLYwuZ9R8Jq/EtnRmQkMy1/IM8s38kb6/ey/cDRkF0O\novLQMe58cTUX/GIJz67YydVFuSz55gX8/OpJ5A9K8vnr9+8XxzdnjeX9rYd4ZY39TQZCj1VkIvKD\nzo6r6r09nFoN5Hk8znWPdVamSkRigDScMTddnisisTjJ5f9U9UWPeE4MpReRPwIvdxH3o8CjAEVF\nRX36tSY6SphZkMmC0l00trQSHxP6c2UFi7XVtaiGb/uLpy+eN4Jbn17JTU+UABAXHUX+oH6MGJTM\nyMwkRmYkMyIjmREZSaQGYVXs9gNHeWRJBS+uqiZahGunDeXW80cypL//qzavnTaU/1u2k5/8ax0X\njsskMc7+Jv3JmzYYz+HpCTiDLL0ZKrsCGC0iw3GSwzzgug5lFgA3AEuBq4HFqqrugmZ/FZEHgSHA\naGC52z7zGLBeVR/0vJCIDFbV9rk2rgLWehFjn5tVmM0zyyt5r+IgHxvnTVOV8Ub7CP5w7UHmaWZB\nFh98byYV+4+wZf8Rtu4/ypb9R9i0r57X1++lpe2/34syU+IZkeEknZFu0hmZkUxO/0Si/DxWaMv+\nIzyyuIJ/lFYTGx3FZ88axi3njSQ7LXBtSdFRwt1XFHDNo+/zu7e28PWZYwIWSyTyZqDlh6qaROSX\nwEIvzmsRkdvdstHAn1W1XETuBUpUdQFOsnjKbcQ/hJOEcMvNx2nvaQFuU9VWEZkBXA+sEZFS96W+\n6/YY+7mITMZZq2Y7cEvPP37fO3tkOsnxMSws32MJpg+VVdaSOyCR9OT4QIfiF2n9Ypk6bABTh324\nKbG5tY2dh46xZd8Rth44ypZYDLIXAAAgAElEQVR9ThJ6efVuahv+O1lmQmwUwwcleyQf59/hg5JI\nivfme6X3Nu2t5zeLK3h59S4SYqK5acZwvnjeCDJTgqOTwpkj0rli0hD+8NYW5kzNJW9gv0CHFDGk\nt41fbuP5CrdrcUgrKirSkpKSPr/uHc+s4r2KAyy/6+KwG3EeKOfct5jJQ/vzyHWnBzqUoKSqHDra\nxBb3bmfr/iMn9isPHcPjpochaQmMaE86mf+988lOTehVB4p1u+p4eMlmXlmzh6S4aD57dj5fmDE8\nKL8E7Kpp4MIH3uRjYzP53WemBjqckCciK1W1qKdy3rTBrOG/K1hGAxlAT+0vEW1WQRYvle3ig52H\nOSN/YKDDCXkHjjRSXdPAjWfnBzqUoCUipCfHk54cz7ThH/6da2xpZcfBj971/O2Dao40tpwolxQX\nfaJtp73KbWRmEvnpSR9ae2dNVS2/XryZ19btJSU+hjsuHMXnzxnOgKQ4v/28vTWkfyK3XTCKB17b\nxHsVBzh71KBAhxQRvLlX9pzYsgXY63YpNl24YGwGcdFRLFy7xxJMH4ik9hdfiI+JZkxWCmOyUj50\nXFXZX9/otvUcPXHXU7L9MAvKdtFeuSECuQMSGZmRTGub8p/NB0hNiOFrF4/hxnPySUsMvo4Gnfni\neSOYv7KSu18q55Uvn2vzlPmBNwmm47QwqZ630ap6qE8jCgMpCbGcMyqdhev2cNfl48N63IY/lFXW\nEiWc0vQh5qNEhMzUBDJTEzh75Ie/0Tc0tbLtwNEPdTLYsv8ItQ3N/E/xWD571rCQG0ycEBvN9y4v\n4JanVvL0+zu48ZzhgQ4p7HmTYD7A6TJ8GBCgP7DTfU6BEb4JLbTNKsxmyYtr2LCn3kYRn6KyqhpG\nZ6b0eeO06VpiXDQFQ1IpGBJev7uzCrKYMWoQD762iSsmDQnK9qJw4s094mvAFao6SFXTcarMFqnq\ncFW15NKFi8dnIWJrxJwqVaWssoZJeXb3Yk6diPDDKwo42tTKAzZPmc95k2Cme04cqar/Bs72XUjh\nISMlnqJhA1hYbkspn4qqww0cPtYc9iP4jf+MzkrhhrPyeWb5TtZW1wY6nLDmTYLZJSLfE5F8d7sL\n2OXrwMLBrIJs1u+uo/LQsUCHErJKK8N7BmUTGF+5eDQD+8Vx9wKbp8yXvEkw1+J0Tf67u2W6x0wP\niguzAasmOxWrq2qIi4libHZKz4WN8VJaYiz/UzyWkh1OjznjG95MdnlIVb+iqlOAC3EmmLSeY14Y\nmt6PcdkpLLJqspNWVllL4ZBUYq1Lqeljc4ryOC0njZ+9soGjjTbywhe6/KsVkR+IyDh3P15EFgMV\nwF4RudhfAYa64sJsVuw4xIEjjYEOJeS0tLaxprrWqseMT0RHCXd/ooA9dcf57ZsVgQ4nLHX3tfAa\nYKO7f4NbNhM4H/ipj+MKG7MKnaWU31hvdzG9VbH/CA3Nrb1el90Yb00dNpCrpuTwx7e3sePg0Z5P\nML3SXYJp0v+2fhUDz6hqq6qux7vxMwYoGJxK7oBE6012EsoqbQS/8b3vXDqOmGjhx//yZpJ40xvd\nJZhGEZkgIhnAx4BFHs/ZdKReEhGKC7N5Z/OBD837ZHpWVlVLakIM+em+X5zKRK6s1ARuv3AUr63b\ny9ubbDXavtRdgvkK8AKwAXhIVbcBiMhlwCo/xBY2ZhVk0dTaxlu2lHKvlFXWMDG3v9/XNTGR56YZ\nwxmW3o97XiqnOURXEA1GXSYYVV2mquNUNV1Vf+Rx/BVVtW7KvVCUP5D0pDjrrtwLx5tb2bin3kbw\nG7+Ij4nmBx8vYMv+ozzx3vZAhxM2rO+nH0RHCRePz2LJhn00tdi3I2+U76qjpU2tB5nxmwvHZXLB\n2Ax+9fpm9tdbr8++YAnGT4onZFHf2MJ7Ww4EOpSQ0N7Ab1PEGH8REb7/8QIamlv55cKNPZ9gemQJ\nxk/OHjmIpLhoFq2z3mTeWF1VQ3ZqAlmpwbHsrokMIzOS+fyM4cxfWXniS445eV4lGBE5W0SuE5HP\ntm++DizcJMRGc8HYTF5bt5e2Npv7qCdlVbXWPdkExB0XjiI9KZ67Xyq3v9VT1GOCEZGngF8CM4Az\n3K3HtZjNR80qzGJ/fSOrKg8HOpSgVnusmW0Hjlr1mAmIlIRYvn3JWFbtrOEfpdWBDiekeTNgsggo\nUJty9JR9bFwmsdHCovK9TB1mSyl3ZXW1UzVhI/hNoHzq9FyeXraTn/17A7MKs0m2xe5OijdVZGuB\nbF8HEglSE2I5a+QgFpbvsSnCu9Fe921LJJtAiYoS7vlEIfvrG/nN4s2BDidkeZNgBgHrRGShiCxo\n33wdWLgqLsxi+8FjbNp7JNChBK2yqlpGZCSRlhhaa76b8DI5rz9XT83lz+9sY9sBm6fsZHiTYO4G\nrsSZ4PIBj82chJm2lHKPyiprbPyLCQrfumQs8THR/OjldYEOJSR5sx7MW51t/gguHGWmJjAlrz+L\n1lmC6cye2uPsq29kkvUgM0EgMyWBr1w0msUb9rFkw75AhxNyvOlFNl1EVojIERFpEpFWEanzR3Dh\nqrgwm7XVdVQdtqWUO2pfInmiNfCbIHHD2fmMyEji3pfX2UwcveRNFdnDOEskbwYSgS8Aj/gyqHA3\ny11K2Va6/KiyqhpiooSCwamBDsUYAOJiovjBxwvYduAof3l3W6DDCSleDbRU1Qog2l0P5i/AJb4N\nK7wNH5TEmKxkqybrxOqqGsYPTiUhNjrQoRhzwgVjM7l4fCa/fmMz++qOBzqckOFNgjkmInFAqYj8\nXES+5uV5phvFhdks33aIQ0ebAh1K0GhrU1ZX2gh+E5y+d3kBza3K/a/aPGXe8iZRXO+Wux04CuQB\nn/JlUJGguDCbNoXXbSnlE7YdPEp9Y4uN4DdBKX9QEjedO5y/fVDFBzttNg5veNOLbAcgwGBVvUdV\nv+5WmZlTUDgklZz+iSyy7sonnJhB2boomyB1+8dGkZUazz0LbJ4yb3jTi+wKoBR41X082QZanjoR\nYWZBFm9vPsBRW0oZcBJMv7hoRmUmBzoUYzqVFB/DnZeOp6yqlhc+qAp0OEHP24GW04AaAFUtBYb7\nMKaIUVyYTVNLm60D7iqrquW0nDSibYlkE8RmTx7C1GED+PmrG6g73hzocIKaNwmmWVVrOxyze8M+\ncEb+AAb0i7VR/UBTSxvrdtVZ+4sJeiLC3VcUcvBoE79+3eYp6443CaZcRK4DokVktIj8BnjPx3FF\nhJjoKC4an8UbG/bR3BrZA7g27qmnqbXN2l9MSDgtN415Z+Tx+Hvbqdhn8wp2xZsEcwdQCDQCzwB1\nwFe9ubiIXCIiG0WkQkS+08nz8SLynPv8MhHJ93juTvf4RhEpdo/licgSEVknIuUi8hWP8gNF5DUR\n2ez+O8CbGAOtuDCb+uMtvL/1YKBDCajSKncEv3VRNiHim7PGkhgXzT0vldvs6F3wphfZMVW9S1XP\nUNUid7/HkUYiEo0z4v9SoAC4VkQKOhS7CTisqqOAh4D73XMLgHk4ie0S4Lfu9VqAb6hqATAduM3j\nmt8B3lDV0cAb7uOgd+7oQSTGRkd8NVlZZQ3pSXHkDkgMdCjGeCU9OZ6vXTyG/2w+wOvrbZ6yznSZ\nYDyn5u9s8+La04AKVd2qqk3As8DsDmVmA0+4+y8AF4mIuMefVdVGVd0GVADTVHW3qn4AoKr1wHog\np5NrPYEzA3TQS4iN5vwxGRG/lPLqqhom5fXH+e83JjRcf9YwRmcm86OX13G8uTXQ4QSd7u5gzgJy\ngf/gLJn8AL2brj8HqPR4XMV/k8FHyqhqC1ALpHtzrludNgVY5h7KUtXd7v4eIKuzoETkZhEpEZGS\n/fuDo/dW8YQs9tY1UuZWE0WaI40tbN53xKrHTMiJjY7ih1cUsvPQMR57x+Yp66i7BJMNfBeYAPwK\nmAkcCIbp+kUkGfgb8FVV/cjMzu7yzp3eDqjqo25VX1FGRoaPI/XOhWOziIkSFkbo5Jdrq2tRxXqQ\nmZA0Y/QgiguzeGRJBXtqbZ4yT10mGHdiy1dV9Qac9o4K4E0Rud3La1fjTCvTLtc91mkZEYkB0oCD\n3Z0rIrE4yeX/VPVFjzJ7RWSwW2YwEDKVomn9Ypk+Ip1FEbqUso3gN6Hue5cX0NKm3Pfv9YEOJah0\n28jv9vL6JPA0cBvwa+DvXl57BTBaRIa7k2XOAzq23SwAbnD3rwYWu3cfC4B57usPB0YDy932mceA\n9ar6YDfXugH4p5dxBoXiwiy2HjjKlv2R1+WxrKqGvIGJDEyKC3QoxpyUvIH9uPW8EfyjdBcl2w8F\nOpyg0V0j/5PAUuB04B63F9mPVLXjXUin3DaV24GFOI3x81W1XETuFZFPuMUeA9JFpAL4Om7PL1Ut\nB+YD63CmqLlNVVuBc3Am37xQRErd7TL3WvcBM0VkM3Cx+zhkzCxw1oiJxGqysspau3sxIe/WC0Yy\nOC2BHy4opzWCO+x4kq6qZESkDWf2ZPhwe4bgNHOE/IpQRUVFWlJSEugwTrjykXdpU2XB7TMCHYrf\nHDjSSNGPX+euy8bzxfNGBDocY07JS2W7uOOZVfzsk6dx7bShgQ7HZ0RkpaoW9VSuuzaYKFVNcbdU\njy0lHJJLMJpVmMXqqlp21TQEOhS/We32nLMGfhMOPj5xMNOGD+QXCzdSe8zmKbOFw4JIsbuU8mvr\nIqearLSyliiBCTn2ncWEvvZ5ymqONfHQ65sCHU7AWYIJIiMzkhmVmRxRo/rLKmsYk5VCv7iYQIdi\nTJ8oGJLKdWcO5an3d7Bpb32gwwkoSzBBZlZBFsu2HeJwBCylrKrOCH5r4Ddh5hszx5IcHxPx85RZ\nggkyxYXZtLYpizeEzDCek1Z5qIHDx5qZmGcj+E14GZAUxzdnjeHdioP8ctHGiE0ylmCCzMTcNLJT\nEyKimqx9ahy7gzHh6NNnDuPaaUN5ZMkW7nlpXUTONWgV30FGRJhVmMX8kkoamlpJjIsOdEg+U1ZZ\nQ3xMFGOzUwIdijF9LipK+OlVE0iOj+aP/9nGkcYW7vvkacRER873+sj5SUNIcWE2x5vbeHtzcEzG\n6StlVTUUDkklNoL+4ExkERG+e9l4vnbxGF5YWcWXn11FU0vkLC5of9lBaNrwgaQlhvdSyi2tbayt\ntiWSTfgTEb5y8Wi+d/l4Xlmzh5ufKqGhKTKm9rcEE4Rio6O4aHwmb6wP36WUN+87QkNzq7W/mIjx\nhXNHcN8nT+OtTfu54S/LqT8e/gMxLcEEqVkF2dQ2NLNiW3hOnGcj+E0kmjdtKL+eN4UPdhzm039a\nFvbDESzBBKnzx2SQEBsVttVkpZW1pCbEkJ/eL9ChGONXV0wawh+un8qGPfVc8+hS9tWF7xoylmCC\nVGJcNOeNzmDRur1h2Ye+rNKWSDaR66LxWTz+uTOoOtzA3D8sperwsUCH5BOWYILYrMJsdtceZ3VV\nbaBD6VPHm1vZuLfe2l9MRDt75CCe/sKZHDraxJzfLw3LtaAswQSxi8dnEh0lLFoXXtVk5btqaW1T\nJubaCH4T2U4fOoDnbjmL5tY25v5+Ket2fWQF+JBmCSaI9e8Xx5nDB4bdImRllc4d2WRr4DeG8YNT\nee6Ws4iLiWLeo0tZueNwoEPqM5ZgglxxYTYV+46E1e1zWVUN2akJZKYmBDoUY4LCyIxknr/1LAYm\nxXH9Y8t4r+JAoEPqE5ZggtzMgiwAFoXRXYzTwG/VY8Z4yh3Qj/m3nkXegH7c+PgKXg+DdaEswQS5\nIf0TmZibFjbdlWuONbH94DEmWgO/MR+RmZLAc7dMZ3x2Crc+vZIFZbsCHdIpsQQTAooLsymtrGFP\nbej3l2/vEWftL8Z0rn+/OJ7+wpmcPmwAX3l2Fc8s3xnokE6aJZgQMMutJnstDHqTtY/gP816kBnT\npZSEWJ743DTOH5PBnS+u4U//2RrokE6KJZgQMCozmRGDklgUBnWypZW1jMhIIjUhNtChGBPUEuOi\nefT6Ii4/bTA//td6/vf1TSE36NoSTAhw1ojJZumWg9QeC90J8lSVsqoaJlv7izFeiYuJ4tfXTmHO\n1Fz+9/XN/ORf60MqyViCCRGzCrNoaVMWbwzdu5g9dcfZX99oAyyN6YXoKOH+T03kxrPz+dM727jz\nxTW0hsjqmJZgQsTk3P5kpsSHdHflskqbQdmYkxEVJfzwigLuuHAUz66o5KvPlYbEUh62ZHKIiIpy\nllL+28pqjje3khAbekspl1XVEhstjB+cGuhQjAk5IsI3Zo0lKT6G+/69gYamFh6+7vSg/iywO5gQ\nUlyYTUNzK//ZHJqjfMsqaxiXnRrUfxDGBLtbzx/Jj6+cwBsb9vH5x1dwtLEl0CF1yRJMCDlzeDop\nCTEhOeiyrU1ZU1VrI/iN6QOfmT6MB+dOYtm2Q3zmsWVB2/nHEkwIiYuJ4qJxmbyxfi8tIVD/6mnr\ngaPUN7bYCH5j+shVU3L57adPp7y6jmseXcr++sZAh/QRlmBCTHFhNoePNbNie2jNuNrewG8j+I3p\nO8WF2Tx2YxE7Dh7jmj8sZVdNQ6BD+hBLMCHmvDEZxMWE3lLKq6tqSIqLZmRGcqBDMSasnDs6g6du\nmsb++kbm/H4p2w8cDXRIJ1iCCTFJ8TGcN3oQr4XYUsqlVbVMyEkjOsqWSDamrxXlD+SZm6fT0NzK\nnD8sZeOe+kCHBFiCCUmzCrOprmmgPERWv2tqaWP9rjqrHjPGhybkpDH/lulECVzz6NIT1dKBZAkm\nBF00LpMoIWSqyTbsqaOptc0a+I3xsVGZKbxw69mkJMRw3R/f5/2tBwMajyWYEJSeHM8Z+QNDYlR/\n5aFj/Ok/2wCsi7IxfpA3sB/P33I2g/sncsOfl7Nk476AxeLTBCMil4jIRhGpEJHvdPJ8vIg85z6/\nTETyPZ670z2+UUSKPY7/WUT2icjaDte6W0SqRaTU3S7z5c8WaMWF2WzcWx9UDXrt2tqUtzbt5wtP\nrOC8Xyzh5dW7uHbaUHL6JwY6NGMiQnZaAs/dPJ1Rmcnc/GQJ/1q9OyBx+CzBiEg08AhwKVAAXCsi\nBR2K3QQcVtVRwEPA/e65BcA8oBC4BPitez2Ax91jnXlIVSe72yt9+fMEm1mFzhoxwVRNVtvQzGPv\nbOOiB9/ihj8vp7SyhtsuGMU7376Qn33yNESsgd8Yf0lPjueZm6czKbc/dzzzAfNLKv0egy/nIpsG\nVKjqVgAReRaYDazzKDMbuNvdfwF4WJxPodnAs6raCGwTkQr3ektV9W3PO51IlTugH4VDUllYvodb\nzh8Z0FjW767jyaU7+MeqahqaWzl9aH++Om8yl0zIJj7GpoUxJlBSE2J58qZp3PLUSr71wmqONrbw\nuXOG++31fZlgcgDPlFkFnNlVGVVtEZFaIN09/n6Hc3O8eM3bReSzQAnwDVX9yGhEEbkZuBlg6NCh\n3v0kQaq4MJuHXt/EvrrjZKYm+PW1m1raWFi+hyeXbmfF9sPEx0Qxe/IQPntWPhNyrK3FmGDRLy6G\nP91QxJefWcU9L63jaGMLt31slF9qFMKpkf93wEhgMrAbeKCzQqr6qKoWqWpRRkaGP+Prc8WF2ajC\na+v919i/t+44D762iXPuX8wdz6xib10jd102nmXfvYifXz3JkosxQSg+JppHrjudq6bk8MtFm7jv\n1Q1+GUfnyzuYaiDP43Gue6yzMlUiEgOkAQe9PPdDVPXEp6yI/BF4+aQjDxFjspIZlt6PheV7+fSZ\nw3z2OqrKsm2HeGrpDhaW76FVlQvGZPDZs/I5f0wGUTZ40pigFxMdxQNzJpEUH80f3tpK4ZA0PjFp\niG9f04fXXgGMFpHhOMlhHnBdhzILgBuApcDVwGJVVRFZAPxVRB4EhgCjgeXdvZiIDFbV9q4SVwFr\nuysfDkSE4sJs/vLuNuqON/f5OvdHG1v4+6pqnlq6g41760lLjOVz5+TzmenDGJae1KevZYzxvago\n4UezJ3Dm8HQuP22wz1/PZwnGbVO5HVgIRAN/VtVyEbkXKFHVBcBjwFNuI/4hnCSEW24+ToeAFuA2\nVW0FEJFngAuAQSJSBfxQVR8Dfi4ikwEFtgO3+OpnCybFhVk8+vZWlmzYx+zJ3jRT9WzL/iM8tXQH\nf1tZRX1jC4VDUvn5pyZyxaQhJMZZo70xoUxEuMLHdy4nXiuU5rPqa0VFRVpSUhLoME5JW5sy7adv\ncObwgTzy6dNP+jqtbcob6/fy5NIdvFNxgNho4bLTBvPZs/I5fWh/62JsjDlBRFaqalFP5WzJ5BAX\nFSXMLMhiQenJLaV88Egjz66o5K/LdlJd08DgtAS+OWsM15wxlIyUeB9FbYyJBJZgwkBxYRbPLN/J\ne1sOcOG4LK/OKa2s4cn3tvPy6t00tbZx9sh0vv/x8Vw8PouY6HDqXGiMCRRLMGHg7JGDSImPYeHa\nvd0mmOPNrbxUtoun3t/B6qpakuKimTctj+unD2N0VoofIzbGRAJLMGEgLiaKC8Zl8vr6vbS26UfW\nXKk8dIynl+3guRWV1BxrZlRmMvfOLuSqKTmk9HHPM2OMaWcJJkwUF2bxUtkuVu44zLThA2lrU/5T\ncYAn39vO4o37iBJh5vgsPnv2MM4akW6N9sYYn7MEEyYuGJtJXHQUL35QxZrqWp5aup3tB48xKDmO\n2z82iuvOHMrgNJvN2BjjP5ZgwkRyfAznjErn2RXO9G9Thw3gazPH2ISTxpiAsQQTRr580WhGZCRz\n1ZQcmxPMGBNwlmDCyJShA5gydECgwzDGGCC8ZlM2xhgTRCzBGGOM8QlLMMYYY3zCEowxxhifsARj\njDHGJyzBGGOM8QlLMMYYY3zCEowxxhifiOgVLUVkP7DjJE8fBBzow3D6isXVOxZX71hcvROsccGp\nxTZMVTN6KhTRCeZUiEiJN0uG+pvF1TsWV+9YXL0TrHGBf2KzKjJjjDE+YQnGGGOMT1iCOXmPBjqA\nLlhcvWNx9Y7F1TvBGhf4ITZrgzHGGOMTdgdjjDHGJyzBGGOM8QlLMD0QkUtEZKOIVIjIdzp5/jwR\n+UBEWkTk6iCK6+sisk5EVovIGyIyLEjiulVE1ohIqYi8IyIFwRCXR7lPiYiKiF+6lnrxft0oIvvd\n96tURL4QDHG5Zea6v2PlIvLXYIhLRB7yeK82iUhNkMQ1VESWiMgq92/ysiCJa5j7+bBaRN4Ukdw+\nDUBVbetiA6KBLcAIIA4oAwo6lMkHJgJPAlcHUVwfA/q5+18CnguSuFI99j8BvBoMcbnlUoC3gfeB\nomCIC7gReNgfv1e9jGs0sAoY4D7ODIa4OpS/A/hzMMSF06D+JXe/ANgeJHE9D9zg7l8IPNWXMdgd\nTPemARWqulVVm4BngdmeBVR1u6quBtqCLK4lqnrMffg+0LffTE4+rjqPh0mAP3qZ9BiX60fA/cBx\nP8TUm7j8zZu4vgg8oqqHAVR1X5DE5ela4JkgiUuBVHc/DdgVJHEVAIvd/SWdPH9KLMF0Lweo9Hhc\n5R4LtN7GdRPwb59G5PAqLhG5TUS2AD8HvhwMcYnI6UCeqv7LD/F4HZfrU24VxgsikhckcY0BxojI\nuyLyvohcEiRxAU7VDzCc/354Bjquu4HPiEgV8ArO3VUwxFUGfNLdvwpIEZH0vgrAEkyYE5HPAEXA\nLwIdSztVfURVRwLfBr4X6HhEJAp4EPhGoGPpxEtAvqpOBF4DnghwPO1icKrJLsC5U/ijiPQPaEQf\nNg94QVVbAx2I61rgcVXNBS4DnnJ/7wLtm8D5IrIKOB+oBvrsPQuGHzCYVQOe3xhz3WOB5lVcInIx\ncBfwCVVtDJa4PDwLXOnTiBw9xZUCTADeFJHtwHRggR8a+nt8v1T1oMf/3Z+AqT6Oyau4cL4NL1DV\nZlXdBmzCSTiBjqvdPPxTPQbexXUTMB9AVZcCCTiTTQY0LlXdpaqfVNUpOJ8VqGrfdYzwdUNTKG84\n39K24txqtzeSFXZR9nH818jfY1zAFJwGvtHB9H55xgNcAZQEQ1wdyr+Jfxr5vXm/BnvsXwW8HyRx\nXQI84e4PwqmKSQ90XG65ccB23IHkQfJ+/Ru40d0fj9MG49P4vIxrEBDl7v8EuLdPY/DHf0Aobzi3\ns5vcD+u73GP34twVAJyB823uKHAQKA+SuF4H9gKl7rYgSOL6FVDuxrSkuw96f8bVoaxfEoyX79fP\n3PerzH2/xgVJXIJTrbgOWAPMC4a43Md3A/f5I55evF8FwLvu/2MpMCtI4roa2OyW+RMQ35evb1PF\nGGOM8QlrgzHGGOMTlmCMMcb4hCUYY4wxPmEJxhhjjE9YgjHGGOMTMYEOwJhwJiKtON14Y4EWnElR\nH1JVf85dZ0xAWIIxxrcaVHUygIhkAn/FmfTwhwGNyhg/sCoyY/xEnRmHbwZuF0e0iPxCRFa4k1ne\n0l5WRL7trptTJiL3uce+6JYtE5G/iUg/EUkRkW0iEuuWSfV8bEwgWYIxxo9UdSvOOh2ZOPNT1arq\nGTgzQnxRRIaLyKU406afqaqTcGadBnhRVc9wj60HblLVepyZBy53y8xzyzX77YcypguWYIwJnFnA\nZ0WkFFgGpONMGHkx8Bd11/NR1UNu+Qki8h8RWQN8Gih0j/8J+Jy7/zngL36K35huWRuMMX4kIiNw\npkPfhzOf1x2qurBDmeIuTn8cuFJVy0TkRpyp8lHVd0UkX0QuAKJVda1vojemd+wOxhg/EZEM4Pc4\nSyArsBD4kkf7yRgRScJZ9+VzItLPPT7QvUQKsNst/+kOl38SpwOB3b2YoGF3MMb4VqJbBdbeTfkp\nnFmIwanaygc+EBEB9uPcobwqIpOBEhFpwlkB8bvA93Gq0va7/6Z4vM7/AT/Gf2ugGNMjm03ZmDAg\nIlcDs1X1+kDHYkw7u4MxJsSJyG+AS3HW/jAmaNgdjDHGGJ+wRn5jjDE+YQnGGGOMT1iCMcYY4xOW\nYIwxxviEJRhjjDE+8a0OCrgAAAAGSURBVP8B3Oo4GstYjWoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(decay_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Decay')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "7229adf9dfcf7abb6fe0366cf0b584d6e73a88e3",
    "colab": {},
    "colab_type": "code",
    "id": "_VCGsktGBEBl"
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "neurons = [512, 512, 64, 1]\n",
    "epochs = 90\n",
    "rate = 0.3 #dropout\n",
    "decay = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "2d81c8cf96acbf7ed362c57167fbaaf1c96e7f8a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 20825
    },
    "colab_type": "code",
    "id": "LR4XPaxmBEBo",
    "outputId": "615aa980-d684-4db7-bd1b-c1013e8b7ad2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 5, 512)            1058816   \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1218 samples, validate on 136 samples\n",
      "Epoch 1/90\n",
      "1218/1218 [==============================] - 5s 4ms/step - loss: 0.2393 - acc: 8.2102e-04 - val_loss: 0.2624 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1218/1218 [==============================] - 0s 109us/step - loss: 0.1085 - acc: 8.2102e-04 - val_loss: 0.0611 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1218/1218 [==============================] - 0s 98us/step - loss: 0.0455 - acc: 8.2102e-04 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0135 - acc: 8.2102e-04 - val_loss: 0.0534 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1218/1218 [==============================] - 0s 77us/step - loss: 0.0254 - acc: 8.2102e-04 - val_loss: 0.0105 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0057 - acc: 8.2102e-04 - val_loss: 0.0096 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1218/1218 [==============================] - 0s 88us/step - loss: 0.0138 - acc: 8.2102e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0048 - acc: 8.2102e-04 - val_loss: 0.0169 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1218/1218 [==============================] - 0s 84us/step - loss: 0.0092 - acc: 8.2102e-04 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0049 - acc: 8.2102e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0059 - acc: 8.2102e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0038 - acc: 8.2102e-04 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0042 - acc: 8.2102e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0033 - acc: 8.2102e-04 - val_loss: 9.1125e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1218/1218 [==============================] - 0s 76us/step - loss: 0.0033 - acc: 8.2102e-04 - val_loss: 8.8762e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0027 - acc: 8.2102e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0028 - acc: 8.2102e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0023 - acc: 8.2102e-04 - val_loss: 8.4581e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0025 - acc: 8.2102e-04 - val_loss: 9.0482e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1218/1218 [==============================] - 0s 83us/step - loss: 0.0022 - acc: 8.2102e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0022 - acc: 8.2102e-04 - val_loss: 9.2833e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 8.4796e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 8.6285e-04 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.5592e-04 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.7822e-04 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1218/1218 [==============================] - 0s 83us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 8.8173e-04 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1218/1218 [==============================] - 0s 86us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.5947e-04 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0021 - acc: 8.2102e-04 - val_loss: 8.8007e-04 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.2107e-04 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.4599e-04 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 8.9104e-04 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.1921e-04 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.6200e-04 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.4763e-04 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.9632e-04 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.6276e-04 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1218/1218 [==============================] - 0s 84us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.7645e-04 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1218/1218 [==============================] - 0s 83us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.2050e-04 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.2172e-04 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1218/1218 [==============================] - 0s 83us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.8614e-04 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.9812e-04 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.9977e-04 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.9466e-04 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1218/1218 [==============================] - 0s 88us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.8220e-04 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.9999e-04 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.7740e-04 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1218/1218 [==============================] - 0s 84us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1218/1218 [==============================] - 0s 76us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.8733e-04 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 8.5739e-04 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1218/1218 [==============================] - 0s 84us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.6661e-04 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1218/1218 [==============================] - 0s 83us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 9.1559e-04 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.5380e-04 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 8.8827e-04 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.8778e-04 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.5846e-04 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1218/1218 [==============================] - 0s 87us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.9138e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.6135e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.5451e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0020 - acc: 8.2102e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.4809e-04 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.7685e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1218/1218 [==============================] - 0s 89us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.9742e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.9916e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1218/1218 [==============================] - 0s 78us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.9445e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.6140e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.7343e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.4818e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.7862e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.4279e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 9.0610e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1218/1218 [==============================] - 0s 82us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.3908e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.3205e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.7802e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.9899e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0019 - acc: 8.2102e-04 - val_loss: 8.3413e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.4522e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1218/1218 [==============================] - 0s 80us/step - loss: 0.0017 - acc: 8.2102e-04 - val_loss: 8.2655e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1218/1218 [==============================] - 0s 85us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.4108e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0017 - acc: 8.2102e-04 - val_loss: 8.4323e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.5555e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1218/1218 [==============================] - 0s 83us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 8.4073e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0018 - acc: 8.2102e-04 - val_loss: 9.5286e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1218/1218 [==============================] - 0s 79us/step - loss: 0.0017 - acc: 8.2102e-04 - val_loss: 8.5385e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1218/1218 [==============================] - 0s 81us/step - loss: 0.0017 - acc: 8.2102e-04 - val_loss: 9.1466e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00135 MSE (0.04 RMSE)\n",
      "Test Score: 0.00205 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_26 (LSTM)               (None, 10, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 10, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_27 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1214 samples, validate on 135 samples\n",
      "Epoch 1/90\n",
      "1214/1214 [==============================] - 5s 5ms/step - loss: 0.2384 - acc: 8.2372e-04 - val_loss: 0.1772 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1214/1214 [==============================] - 0s 148us/step - loss: 0.0857 - acc: 8.2372e-04 - val_loss: 0.0346 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0293 - acc: 8.2372e-04 - val_loss: 0.1215 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0745 - acc: 8.2372e-04 - val_loss: 0.1083 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0479 - acc: 8.2372e-04 - val_loss: 0.0189 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0131 - acc: 8.2372e-04 - val_loss: 0.0069 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0296 - acc: 8.2372e-04 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1214/1214 [==============================] - 0s 145us/step - loss: 0.0150 - acc: 8.2372e-04 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1214/1214 [==============================] - 0s 144us/step - loss: 0.0183 - acc: 8.2372e-04 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0196 - acc: 8.2372e-04 - val_loss: 0.0249 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0130 - acc: 8.2372e-04 - val_loss: 0.0084 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0122 - acc: 8.2372e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1214/1214 [==============================] - 0s 142us/step - loss: 0.0110 - acc: 8.2372e-04 - val_loss: 0.0075 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1214/1214 [==============================] - 0s 142us/step - loss: 0.0074 - acc: 8.2372e-04 - val_loss: 0.0137 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0079 - acc: 8.2372e-04 - val_loss: 0.0082 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0052 - acc: 8.2372e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0050 - acc: 8.2372e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0037 - acc: 8.2372e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0041 - acc: 8.2372e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0033 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0033 - acc: 8.2372e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1214/1214 [==============================] - 0s 143us/step - loss: 0.0034 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0031 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0029 - acc: 8.2372e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0030 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1214/1214 [==============================] - 0s 141us/step - loss: 0.0031 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1214/1214 [==============================] - 0s 143us/step - loss: 0.0029 - acc: 8.2372e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1214/1214 [==============================] - 0s 133us/step - loss: 0.0028 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0028 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0028 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1214/1214 [==============================] - 0s 141us/step - loss: 0.0028 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0028 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1214/1214 [==============================] - 0s 148us/step - loss: 0.0029 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1214/1214 [==============================] - 0s 147us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1214/1214 [==============================] - 0s 143us/step - loss: 0.0026 - acc: 8.2372e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0027 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0026 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1214/1214 [==============================] - 0s 142us/step - loss: 0.0026 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0026 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1214/1214 [==============================] - 0s 141us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0026 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1214/1214 [==============================] - 0s 143us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0026 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1214/1214 [==============================] - 0s 142us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1214/1214 [==============================] - 0s 150us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1214/1214 [==============================] - 0s 134us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1214/1214 [==============================] - 0s 143us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1214/1214 [==============================] - 0s 144us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1214/1214 [==============================] - 0s 137us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0025 - acc: 8.2372e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1214/1214 [==============================] - 0s 148us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1214/1214 [==============================] - 0s 149us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1214/1214 [==============================] - 0s 145us/step - loss: 0.0022 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1214/1214 [==============================] - 0s 142us/step - loss: 0.0022 - acc: 8.2372e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0024 - acc: 8.2372e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1214/1214 [==============================] - 0s 141us/step - loss: 0.0022 - acc: 8.2372e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1214/1214 [==============================] - 0s 139us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0022 - acc: 8.2372e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0021 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0022 - acc: 8.2372e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1214/1214 [==============================] - 0s 154us/step - loss: 0.0023 - acc: 8.2372e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1214/1214 [==============================] - 0s 145us/step - loss: 0.0021 - acc: 8.2372e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1214/1214 [==============================] - 0s 140us/step - loss: 0.0021 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1214/1214 [==============================] - 0s 150us/step - loss: 0.0020 - acc: 8.2372e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1214/1214 [==============================] - 0s 142us/step - loss: 0.0020 - acc: 8.2372e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1214/1214 [==============================] - 0s 138us/step - loss: 0.0021 - acc: 8.2372e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1214/1214 [==============================] - 0s 136us/step - loss: 0.0021 - acc: 8.2372e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1214/1214 [==============================] - 0s 135us/step - loss: 0.0022 - acc: 8.2372e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00200 MSE (0.04 RMSE)\n",
      "Test Score: 0.00497 MSE (0.07 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_28 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_29 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 6s 5ms/step - loss: 0.2218 - acc: 8.3056e-04 - val_loss: 0.0067 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0153 - acc: 8.3056e-04 - val_loss: 0.0385 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0132 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0120 - acc: 8.3056e-04 - val_loss: 0.0280 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0110 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0082 - acc: 8.3056e-04 - val_loss: 0.0165 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0072 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 9.1872e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.7453e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.9668e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.5722e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.8736e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 9.0562e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.2507e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 9.4854e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.6299e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 7.6890e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.5209e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.0592e-04 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 9.6376e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 7.3074e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.1884e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 9.6600e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.7550e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.3265e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0014 - acc: 8.3056e-04 - val_loss: 6.9004e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.0913e-04 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 9.9727e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 6.6049e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00108 MSE (0.03 RMSE)\n",
      "Test Score: 0.00129 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_30 (LSTM)               (None, 60, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 60, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1173 samples, validate on 131 samples\n",
      "Epoch 1/90\n",
      "1173/1173 [==============================] - 7s 6ms/step - loss: 0.2297 - acc: 8.5251e-04 - val_loss: 0.0297 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1173/1173 [==============================] - 1s 719us/step - loss: 0.0398 - acc: 8.5251e-04 - val_loss: 0.0551 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1173/1173 [==============================] - 1s 724us/step - loss: 0.0202 - acc: 8.5251e-04 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1173/1173 [==============================] - 1s 719us/step - loss: 0.0226 - acc: 8.5251e-04 - val_loss: 0.0199 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0152 - acc: 8.5251e-04 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1173/1173 [==============================] - 1s 727us/step - loss: 0.0143 - acc: 8.5251e-04 - val_loss: 0.0045 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0114 - acc: 8.5251e-04 - val_loss: 0.0047 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1173/1173 [==============================] - 1s 716us/step - loss: 0.0066 - acc: 8.5251e-04 - val_loss: 0.0121 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1173/1173 [==============================] - 1s 732us/step - loss: 0.0050 - acc: 8.5251e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1173/1173 [==============================] - 1s 723us/step - loss: 0.0053 - acc: 8.5251e-04 - val_loss: 0.0087 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1173/1173 [==============================] - 1s 741us/step - loss: 0.0050 - acc: 8.5251e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1173/1173 [==============================] - 1s 722us/step - loss: 0.0048 - acc: 8.5251e-04 - val_loss: 0.0063 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0049 - acc: 8.5251e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1173/1173 [==============================] - 1s 737us/step - loss: 0.0043 - acc: 8.5251e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1173/1173 [==============================] - 1s 730us/step - loss: 0.0036 - acc: 8.5251e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1173/1173 [==============================] - 1s 730us/step - loss: 0.0033 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0029 - acc: 8.5251e-04 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1173/1173 [==============================] - 1s 720us/step - loss: 0.0030 - acc: 8.5251e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1173/1173 [==============================] - 1s 733us/step - loss: 0.0029 - acc: 8.5251e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1173/1173 [==============================] - 1s 731us/step - loss: 0.0030 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0028 - acc: 8.5251e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0026 - acc: 8.5251e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1173/1173 [==============================] - 1s 729us/step - loss: 0.0026 - acc: 8.5251e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1173/1173 [==============================] - 1s 726us/step - loss: 0.0026 - acc: 8.5251e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1173/1173 [==============================] - 1s 730us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1173/1173 [==============================] - 1s 735us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1173/1173 [==============================] - 1s 733us/step - loss: 0.0025 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1173/1173 [==============================] - 1s 729us/step - loss: 0.0025 - acc: 8.5251e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1173/1173 [==============================] - 1s 727us/step - loss: 0.0025 - acc: 8.5251e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1173/1173 [==============================] - 1s 738us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1173/1173 [==============================] - 1s 731us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1173/1173 [==============================] - 1s 739us/step - loss: 0.0023 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1173/1173 [==============================] - 1s 738us/step - loss: 0.0022 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0023 - acc: 8.5251e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1173/1173 [==============================] - 1s 739us/step - loss: 0.0023 - acc: 8.5251e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1173/1173 [==============================] - 1s 751us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1173/1173 [==============================] - 1s 738us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1173/1173 [==============================] - 1s 744us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1173/1173 [==============================] - 1s 735us/step - loss: 0.0023 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0024 - acc: 8.5251e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1173/1173 [==============================] - 1s 737us/step - loss: 0.0025 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1173/1173 [==============================] - 1s 737us/step - loss: 0.0022 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1173/1173 [==============================] - 1s 740us/step - loss: 0.0021 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0021 - acc: 8.5251e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1173/1173 [==============================] - 1s 743us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1173/1173 [==============================] - 1s 745us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1173/1173 [==============================] - 1s 738us/step - loss: 0.0021 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1173/1173 [==============================] - 1s 741us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1173/1173 [==============================] - 1s 726us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1173/1173 [==============================] - 1s 726us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1173/1173 [==============================] - 1s 732us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1173/1173 [==============================] - 1s 736us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1173/1173 [==============================] - 1s 743us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1173/1173 [==============================] - 1s 723us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1173/1173 [==============================] - 1s 733us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1173/1173 [==============================] - 1s 725us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1173/1173 [==============================] - 1s 731us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1173/1173 [==============================] - 1s 731us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1173/1173 [==============================] - 1s 740us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 9.7731e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1173/1173 [==============================] - 1s 731us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1173/1173 [==============================] - 1s 740us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 9.7173e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1173/1173 [==============================] - 1s 732us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1173/1173 [==============================] - 1s 746us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 9.9716e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1173/1173 [==============================] - 1s 735us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1173/1173 [==============================] - 1s 729us/step - loss: 0.0017 - acc: 8.5251e-04 - val_loss: 9.4855e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1173/1173 [==============================] - 1s 739us/step - loss: 0.0017 - acc: 8.5251e-04 - val_loss: 9.7737e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1173/1173 [==============================] - 1s 727us/step - loss: 0.0016 - acc: 8.5251e-04 - val_loss: 9.3292e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1173/1173 [==============================] - 1s 724us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 9.0252e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1173/1173 [==============================] - 1s 734us/step - loss: 0.0017 - acc: 8.5251e-04 - val_loss: 8.7749e-04 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1173/1173 [==============================] - 1s 726us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 8.5927e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1173/1173 [==============================] - 1s 732us/step - loss: 0.0016 - acc: 8.5251e-04 - val_loss: 8.9065e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1173/1173 [==============================] - 1s 741us/step - loss: 0.0017 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1173/1173 [==============================] - 1s 727us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 8.9370e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1173/1173 [==============================] - 1s 743us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1173/1173 [==============================] - 1s 730us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1173/1173 [==============================] - 1s 731us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1173/1173 [==============================] - 1s 725us/step - loss: 0.0022 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1173/1173 [==============================] - 1s 729us/step - loss: 0.0020 - acc: 8.5251e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1173/1173 [==============================] - 1s 727us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0019 - acc: 8.5251e-04 - val_loss: 9.7192e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1173/1173 [==============================] - 1s 726us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1173/1173 [==============================] - 1s 737us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 8.6668e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1173/1173 [==============================] - 1s 719us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 8.9026e-04 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1173/1173 [==============================] - 1s 730us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1173/1173 [==============================] - 1s 719us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 7.8303e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1173/1173 [==============================] - 1s 728us/step - loss: 0.0018 - acc: 8.5251e-04 - val_loss: 7.7377e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1173/1173 [==============================] - 1s 724us/step - loss: 0.0017 - acc: 8.5251e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1173/1173 [==============================] - 1s 723us/step - loss: 0.0016 - acc: 8.5251e-04 - val_loss: 8.6366e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1173/1173 [==============================] - 1s 723us/step - loss: 0.0015 - acc: 8.5251e-04 - val_loss: 7.5744e-04 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00120 MSE (0.03 RMSE)\n",
      "Test Score: 0.00149 MSE (0.04 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_32 (LSTM)               (None, 120, 512)          1058816   \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 120, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1125 samples, validate on 125 samples\n",
      "Epoch 1/90\n",
      "1125/1125 [==============================] - 8s 7ms/step - loss: 0.2485 - acc: 0.0000e+00 - val_loss: 0.0115 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0289 - acc: 0.0000e+00 - val_loss: 0.0501 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0169 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0196 - acc: 0.0000e+00 - val_loss: 0.0270 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0137 - acc: 0.0000e+00 - val_loss: 0.0228 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0096 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0137 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0053 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 9.7599e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 9.7869e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 9.7315e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 9.7785e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 9.8327e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0016 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 8.7384e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 8.5658e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1125/1125 [==============================] - 2s 2ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 8.3250e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0018 - acc: 0.0000e+00 - val_loss: 8.1208e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1125/1125 [==============================] - 2s 1ms/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00111 MSE (0.03 RMSE)\n",
      "Test Score: 0.00205 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_34 (LSTM)               (None, 180, 512)          1058816   \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 180, 512)          0         \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1076 samples, validate on 120 samples\n",
      "Epoch 1/90\n",
      "1076/1076 [==============================] - 9s 9ms/step - loss: 0.2559 - acc: 0.0000e+00 - val_loss: 0.1107 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.1133 - acc: 0.0000e+00 - val_loss: 0.1094 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0464 - acc: 0.0000e+00 - val_loss: 0.0625 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0218 - acc: 0.0000e+00 - val_loss: 0.0141 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0236 - acc: 0.0000e+00 - val_loss: 0.0166 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0212 - acc: 0.0000e+00 - val_loss: 0.0480 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0213 - acc: 0.0000e+00 - val_loss: 0.0339 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0193 - acc: 0.0000e+00 - val_loss: 0.0182 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0194 - acc: 0.0000e+00 - val_loss: 0.0167 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.0155 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0478 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0205 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.0111 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0266 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0117 - acc: 0.0000e+00 - val_loss: 0.0083 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0098 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0066 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0093 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0143 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0111 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0066 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0037 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0062 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0036 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1076/1076 [==============================] - 2s 2ms/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1076/1076 [==============================] - 3s 2ms/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00210 MSE (0.05 RMSE)\n",
      "Test Score: 0.00212 MSE (0.05 RMSE)\n"
     ]
    }
   ],
   "source": [
    "seq_len_list = [5, 10, 22, 60, 120, 180]\n",
    "\n",
    "seq_len_result = {}\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    shape = [4, seq_len, 1]\n",
    "    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs, decay)\n",
    "    seq_len_result[seq_len] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "c63c2d0060330fb5cd1fcfec2856ca006a1d72ce",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "cvnTSxqFBEBq",
    "outputId": "26289cc3-002b-4b26-a9cc-926608a85fb5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxP8PjojVRs0BB4MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lists = sorted(seq_len_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "f341c0a0ff885e2e67559b066dfab9d26deef6d1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3485
    },
    "colab_type": "code",
    "id": "5Zw2Yr4hBEBs",
    "outputId": "38a43c3b-05c9-42e3-b539-97b85d6fc688"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_36 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 8s 6ms/step - loss: 0.2179 - acc: 8.3056e-04 - val_loss: 0.0118 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0169 - acc: 8.3056e-04 - val_loss: 0.0327 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0121 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0114 - acc: 8.3056e-04 - val_loss: 0.0229 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0093 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0075 - acc: 8.3056e-04 - val_loss: 0.0124 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0058 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0043 - acc: 8.3056e-04 - val_loss: 0.0064 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0038 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0039 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0038 - acc: 8.3056e-04 - val_loss: 0.0051 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0035 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0034 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0032 - acc: 8.3056e-04 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 297us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.2765e-04 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 9.8064e-04 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.6473e-04 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 298us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.8286e-04 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.7489e-04 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 292us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.8359e-04 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 8.9039e-04 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 293us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.5975e-04 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.4006e-04 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 7.8827e-04 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.8128e-04 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 300us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 9.7503e-04 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.7137e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.0967e-04 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 296us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 294us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 7.4475e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 7.3853e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 7.5054e-04 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 7.1472e-04 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 289us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 291us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 295us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 7.8268e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 6.9208e-04 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 8.2494e-04 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 290us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 7.3826e-04 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0015 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00166 MSE (0.04 RMSE)\n",
      "Test Score: 0.00421 MSE (0.06 RMSE)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 22\n",
    "shape = [4, seq_len, 1]\n",
    "    \n",
    "trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs, decay)\n",
    "seq_len_result[seq_len] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "5719a4792cac4ed369f966c17ff0f1ab91ce0fe7",
    "colab": {},
    "colab_type": "code",
    "id": "Myj5GCr0BEBw"
   },
   "outputs": [],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-w5_HD_hS876"
   },
   "outputs": [],
   "source": [
    "#p, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "9dbeb1a0d3214d38d1151d52d64a3bd69c689d6d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Ri76Nml6BEBy",
    "outputId": "fbbaef29-355f-4d28-b93e-c523c473fbe2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XucTfX+P/DX2yVDRBhyyyiichka\nIoVI1JG7cLpQpJsj1eHQvU6dVBJ9JT9RVNKpKZSjSHK6UWdEEeOWEbkNBgkZ5v374722PTP2zOw9\ns2evPbNez8djP/bea6/LZ6+Zvd7rcxdVBREReVcJtxNARETuYiAgIvI4BgIiIo9jICAi8jgGAiIi\nj2MgICLyOAYCChsReUJE3nY7HdFKROJEREWkVAjb9BKR7SJyRESaF2b6yLsYCChozsXI98gQkWOZ\n3t8U5mPNFJGnw7CfkC++kSIiy0RkaB6rjQcwXFXLq+qqcB5TRDo452ZKtnW+FpHBzuvBInIq299+\nsogMFZH1IlIm03ZVRGSviHQtaDopshgIKGjOxai8qpYH8CuAGzItm+12+sIlyoJGXQA/52dDESkZ\nxGp/ALhFROJyWWd55r+9qg5X1ekAfgPwWKb1JgJYqKqf5ie95B4GAgq3s0TkTRH5XUR+FpEE3wci\nUlNEPhCRVBHZKiIjAu1ARIYBuAnAaOcO9OO8theRViKSJCKHRWSPiExwPvrSeT7o7KtNgOM9ISKJ\nIvK2iBwGMFhESojIGBHZIiL7ReQ9EansrB/jrLtfRA6KyP9EpLrzWYqIXJNt32cUl4nIMwCuAjDZ\nd5ed7fMyInIEQEkAP4rIFmf5xc5d/UHn/HbPtM1MEXlVRBaKyB8Ars7pj5TJQQAzATwexLrZDQVw\nj4jEi0gXAJ0A3J+P/ZDLGAgo3LoDeBdAJQAfAZgMACJSAsDHAH4EUAt20RjpXECyUNVpAGYDeN65\nA70hiO0nAZikqucAuBDAe87yds5zJWdfy3NIdw8AiU66ZwP4G4CeANoDqAkgDcArzrqDAFQEUAdA\nFQB3ATgW7AlyvuPDAL6Cv9hneLbP/3RyXgDQTFUvFJHSzjlYDKCak8bZItIw06Z/BfAMgAoAvg4y\nOc8A6JNtP8F8hxRYjuB1AFMB3KOqaaHsg6IDAwGF29equlBVTwF4C0AzZ3lLALGq+pSqnlDVXwC8\nBmBAkPvNa/t0APVFpKqqHlHVFSGme7mqzlPVDFU9Bru4P6yqO1T1TwBPAOjrFBulwwJAfVU9paor\nVfVwiMfLj9YAygMY55yDpQAWABiYaZ35qvqN8z2OB7NTVd0Nu5A/ldNxnRyI79E602eTYedjtarO\nC/kbUVSIprJQKh52Z3p9FECMc/GsC6CmiBzM9HlJ2F1xMPLafgjsQpYsIlsBPKmqC0JI9/YAx5sr\nIhmZlp0CUB0W4OoAeFdEKgF4GxY00kM4Xn7UBLBdVTOnaRssh+ST/XsE6zkAW0SkWYDPVqjqlYE2\nUlUVkfUAduTzuBQFGAgoUrYD2KqqDYJcP/uwuLlur6qbAAx0ipB6A0gUkSoB9hPK8W5X1W9yWP9J\nAE86lawLAWwAMANW+Vou03rnhXDMvOwEUEdESmQKBucD2FiAfdpGqvtFZCKAf+ZneyraWDREkfI9\ngN9F5B8iUlZESopIYxFpmcP6ewBcEOz2InKziMQ6F0hfriEDQKrznHlfwZgK4BkRqevsP1ZEejiv\nrxaRJk6rnMOwohHfhXk1gAEiUtqpKO+byzGyf8e8fAfLZY129t8BwA2wOplwmADgCgAXh2l/VEQw\nEFBEOHUG3QDEA9gKYB+A6bBK10BmALjEKZOeF8T2XQH87LS0mQRggKoeU9WjsMrQbwKUb+dmEqyy\ne7GI/A5gBYDLnc/Og1UsHwawHsB/YcVFAPAorLI6DZZreCePY/QVkTQReTmvBKnqCdiF/zrY958C\n4FZVTQ7yO+W1/8MAngdQORz7o6JDODENEZG3MUdARORxDARERB7HQEBE5HEMBEREHlck+hFUrVpV\n4+Li3E4GEVGRsnLlyn2qGpvXekUiEMTFxSEpKcntZBARFSkisi2Y9Vg0RETkcQwEREQex0BARORx\nDARERB7HQEBE5HEMBEREHsdAQETkcQwERAW0bRswj5M0UhFWJDqUEUWrzZuB9u2BnTvtUaOG2yki\nCh1zBFTkZGTkvU4k/PILcPXVwP799v5//3M3PUT5xUBARcaBA0DPnkBcHPDnn+6mZdcu4JprgKNH\ngWXLgBIlAI6CQkUVi4aoSFi1CujVy8rjAWDtWuCyy9xJy8GDQNeuwN69wNKlQKtWwKWXMkdARRdz\nBBT1fvgB6NjRioT+/W9b5uZF94EHgPXrgQ8/tCAAAC1bWo6AM79SUcRAQK45fhzo3Bno0wc4fNi/\n/NdfgW7dgLp1gb/+1YpgKlYEvvwS6NcPqFIlssUw6enAiRP2evduYPZsYNgw4Npr/eskJAD79vlz\nLERFCQMBuUIVGD4cWLIEmD8faNsWeOstYOxYoEkTK3dv0cKKXqpXB774wuoGROyiG4kcwc6dwMMP\nA7VqAc2bW7CaMsUCw333ZV03IcGeWU9ARRHrCMgVr70GzJgBPPSQFfv07QvceqtVunbqBEydClxw\ngb+oRcS/bcuWwLPPWkVtuXIFT8vYscCmTUC1ahaQ+vYFFi0CBg2yi3/nzhawbr4ZWL7ccisNGmTd\nR9OmQOnSFqD69i14mrI7eBAoWRKoUCH8+yaCqkb947LLLlMqPpYvVy1dWrVLF9WTJ23Z3r2qq1er\nHjuW9/bz5qkCqt9+W/C0rFtn+6pZU/Xcc+115cr23KKF6oYNtt7EibYMUF26NPC+LrtMtWPHgqcp\nkMsvV23Z0n++iIIBIEmDuMayaIgiavduqxOoUwd45x27ywWA2FigWTMgJibvfbRsac+hFA8dPmzF\nPCkpWZe/8QZQqpRVSO/bByxebHUSo0cD33wDXHSRrTdiBHDvvcD11wMdOuScrqSk8Pdz2LED+O47\n+74zZoR330QAmCOgvKWmqr75puru3fnbPjlZdfRo1d69VS+4QLVsWbv7L4gaNVRvuSW4dffutbt1\nQHXwYP/yEydUq1VT7dmzYGnxef99O8Yjj4Rnfz5Tp9p+Gza03Mq+feHdPxVfYI6AwkEVGDzYyu9r\n1rTy8SNHgtv26FGgRw+gUSPgpZeADRusJdD779vdf0G0bJk1R6BqlbjZHT4MtGsH/Pwz0KaNNT89\neNA+W7jQ+gLcfnvB0uLTpw8wZAjw9NPAm2+GZ58A8PHHVl/y3nvAoUPAE0+Eb99EAJgjoNwlJtrd\n6KhRqmPH2uuxY4PbduhQW/+JJ/Kfm8jJP/9p+y5b1u7qS5dWLVdOde7cwOstWaKalGSvX3nFPuve\nXfW881TT08OXrj//VL36akvP+vUF398ff6jGxKiOGGHvBw5UjY1VPXWq4Pum4g9B5ghcv8gH82Ag\ncMfBg1YEEx/vv1gOGmQXOV8lak5mzw4taIRq3z7VF15QffBB1TvuUP3HP6wytVQpq0xWVT182CqA\nu3Xzb9e8uWrTpqrPP2/pe/jh8Kdtzx7VChVUe/Uq+L4+/tjSuXixvX/jDXv/008F37eq6s6dqrVq\nqX7/fXj2R9GFgYAKbNQoVZGsF4ldu1TPOcda/GRkZF3/5EnVl19W7dpVtUwZ1bZtw3u3nZeDB1Vb\ntbJA9cILqk8/bf/hmdP/6qt6uvVP//52B18YfDmRgrZsuvNO1fLl/elMSbH9TpyYdb3Dh1W3bAl9\n/++9Z/t78cWCpZOiEwMBFciePVbUctNNZ342fvyZF1hV1WeeseUXX6x6770WNCItLc0qf30X++uu\ny/r5oUOq9eqpDh9euE0xjxxRrV5d9aqrzgyY2WVkqE6YYE1ZM1uzxnIW/fplXX7hhVasldmQIbZu\nWlpo6Rwzxs7T3XeHth0VDQwEVCCjR1tuIFA5d2qqaokSqo895l+WlGTFMv37533hK2wZGapvv21F\nWqtWBf48Eny5j0WLcl8vOdnWu+QS1ePHbdnu3ap161rR3PbtWdcfNsxyZb7c1tGjFgQA1ZdeCi2N\nXbrYdtdcE9p2VDQwEFC+paaqnn22VUzmpG1b63ClaheiRo2srPnAgciksSg4ftwu5HldZH05LED1\noYes/qV5c8uRJSWduf6772bNkfkq9KtUUa1fP/iK5IwMq2gHVOPiQvtuVDQEGwjYfJSyULUhF44e\nBR59NOf1unWzTli//WbDQSQnA6+/Dpx7buTSGu3KlLExiZYssWG0162z87ZxY9b1Fiyw8ZUGDwae\ne86a1qakAImJgYfa9nVoW7rUnufMsfGYJk2yGdM+/TS49O3aZc1nq1Sxgf7cnuOBXBRMtHD7wRxB\n5PiGUhg9Ovf11q7V05WW551XeEMrFHVpaVbZ27Gj1Rlk79SWlqZasqS1rjpwwDqN9ehhrXly07ix\nFX1t2WIV8yNGWIVyjRqWW1u2LO/6ggULLD133GHPyckF/74UXcCiIZo+3S7YwfrkEyv779kz7+KF\njAwrTihXzv6L/vvfgqW1OHvgATtH551nTVnLlLHezqr+Yh5f66Jg6y/efNNaR511lm2/fLktnzTJ\nX8xUsaJVWufE17Lp00/tecGC/H9Hik7BBgIWDRVTK1cCQ4cCzz8f/DYPPWSjar79to0CmhsRK+Y4\netSKKtq1K1Byi7XRo63YZ+lSYNw4K4KZPt0+W7AAqFrVP8FN5lFWc3PLLdaz+pJLrCjp8stt+YgR\nwPbtwPjx1gt55cqc97FqFVC/vg33DdgIrORNhRYIROR1EdkrImszLassIp+JyCbnmSXKhWTcOHsO\ndCF44w3g3XezLlu50i4MI0YAZ58d3DFuvNEGjXvyyYKltbirXt3O+cUX25SWHTsCr74KfPAB8J//\n2EB2vsH3QtGsmdXTJCVlDSC1a9uQIIANVpeTVatsnoWqVW3in82bQ08DFQ+FmSOYCaBrtmVjAHyu\nqg0AfO68pzBLTraLTOXKNqXiH3/4Pzt0yEbRHDrURgL1mTYNKFsWuOmm4I9z1VVAWhpzA6H629/s\nrr1vXxv59N57878vEdtHdrGxQL16wPffB97u4EFg61YLBCKWM2Ag8K5CCwSq+iWAA9kW9wAwy3k9\nC0DPwjq+12RkAP/6l03Y8ve/23DOzz9vy1ev9q/3zjvAsWNWpPPUU7bsyBFb3r+/3RmGghOlhK57\nd+CVV6w10c6d/mKhcLv88pwDwTff2LOvVRIDAXDypLXaql/fiuxCpQqsXWuDDvrOb5ERTEVCfh8A\n4gCszfT+YKbXkvl9bg9WFudtxQp/JSGgOnKk6m+/2etJk2ydjAxradKsmeo991hrlVWr/EMxfPON\nu9+BwmvCBPu7BurhffPNqpUq+TuwPfyw/T+cOBHZNLolJUX1tdf87/fsscl/ABvUT8T6d4TS+7xH\nD//vr1On8Kc5PxDtlcVOIjWnz0VkmIgkiUhSampqBFMWfY4dAx54wNp7b9gQeJ1586yIICXFyvuf\nfdaGja5Rwz+P7sqVljsYNgx47DHLNTRvDjzyiN09tmkTsa9EEeDLaWTPFRw9Csyda0VTZcrYsvr1\ngVOnzpy4p7iaMQO44w4bnhwAJk+2yvd33rFz0Lu35axr1rSivLyGXt+61ebevvNOy1knJfmnWS0K\nIh0I9ohIDQBwnvfmtKKqTlPVBFVNiI2NjVgCo80vvwDx8Taef1pazuPcz5tnrXfq1rVWIL6Zvi67\nzF9h/Npr/nqA6tWBWbOAMWOATz6xFi3BtlihoqF5c6uEzl5hvGCB1Rv99a/+Zb45mL1SPLRzpz0n\nJvqf27cHBg60ebDfe8/q2dq3tyDx0ku572/RInu+/36b4e7QoaJ1LiMdCD4CMMh5PQjA/Agfv0g5\ndcpaf+zda+XJnTrZP2j2O43kZHv0DFDjkpBgFcbLl1vLlZtu8tcD9OljOYeuXcMzCTxFl3LlgKZN\nz8wRvPOO5RQzV/I3bGjPa9ZELn1u8gWC99+3XMH69ZZD8ilRwnIF770HdOlivecDTXzk88knQFyc\nTW2akGDLfDnxoqAwm4/OAbAcQEMR2SEiQwCMA9BZRDYBuMZ5TzmYPNkqnSZNsiDQv7/dZaxalXW9\n+U447d79zH0kJFjg6NEDqFTJLvzkHa1aWSDwzaOclmYXrQEDsjZZrVrVcgXffutOOiNt507LAf/8\nM/DPf9rr3r0Drzt8uK3/4YeBPz9xAvj8c+C662w/l15qOfJQ5tR2XTAVCW4/vFhZ/Msv1mv3+uv9\nvU3377cRPrMP/9C6tWpCQuD97Nrlr8B6773CTTNFH99ENj/+aO/fftver1hx5rqDBllFqdujx0ZC\nbKzqDTf4fxvt2uW87smTNtf2lVcG/vzzz20f8+f7l7VubUOQuw3RXllMuZs+3Xqg/r//5y+7r1wZ\n6Nw5a/FQSgqwYkXgYiEAOO88mzN4wACgX7+IJJ2iSKdO9rxkiT1/9pk1OmjZ8sx127YFUlOLVtl2\nfpw4Yd8zIcG+M5C1WCi7kiWtr8fXX/tz4xkZNjjj66/bnNKlS1tHQZ+WLa2z36lThfc9womBIEot\nWmSteGrXzrr8xhvt4u8befLFF+2f0NeTNJAffrBhI8h76tSx8v/PPrObh8WLLTgEGkLkiivsuci1\ngQ/Rnj32XLOm/W7Kls25WMjnttusz4yvaHXuXOu9P2QIMHGida4sX96/fkKCVcgnJxfOdwg3BoIo\nlJpqF+9rrz3zsxtvtJZBvt6p06fbP3OdOjnvr2zZ/A1hQMVD587Af/9rd7O7dtn7QC6+2OqRinsg\n8FUU16xpTUh37ABq1cp9m3PPtd9cYqJ1GnvqKasYnjfPAuvw4VnX9+W4iko9AQNBFFqyxO7eunQ5\n87Ny5YApU6yVQ7t2Vnw0enTk00hFx7XXWl8UX0/ynAJBiRKWK4h0hfG6dTZu0rp1kTmeLxDUqGHF\nrpUrB7fdAw/YOFw9ewI//WT9b3r0sN9rr15Z173oIsshrFgR3rQXFgaCKLRokf1zBpqUBLBByvr1\nsyKifv3sn44oJx06WGfD+fOtZVDdujmve8UVdkFevNi2e++9wk/fM8/YhfXeeyPTCStzjiAUVarY\nnf+WLdYBb+DAnNctWdJ+p6+9FplzWFAMBFHGV47buXPuxTmTJlkF8NNPRy5tVDRVqAC0bm2vc8oN\n+PgqT7t0seKku+8G9u8vvLRt2wb8+99Wj7Fsmb0ubDt32m8rP/1UH3zQbrzGjQs82F9mM2bY+fzr\nX3NuehotGAhClFunknBYs8bKcQPVD2RWo4ZNUejrEUqUG18AyCsQtGplrcxuuw346ivrIZvblKUF\nNXGiFc8sWmQ94h980MrgC9OuXfb7yWvOjUCqVrVhXvr0yXvd8uWBhQut4njoUOvDEa0YCEKwfj1Q\nrZp/UpHC8Mkn9pxXICAKxe232919oHqnzMqVs//z118HrrzSikKmTrXGC/mRW1FPWpoVnQwcaMVV\nU6ZY7qNJE3sU1kQ5O3eGXiyUX+XLWxPwgwdtZNNoxUAQgmeftT/oyJGFNzjXu+/aAHDZm40SFUTt\n2nahLVs2tO2eeMJufm691QarC8WLL1qONadc9MiRts+//93eX365FRVNmWJjbIUyu14odu60HEGk\nNGsG3HyzFefu2JH3+keOWH3OoUOFnzYfBoIgpaTYGC033mhZ2WHDwl+xtX69jQ6aeTAwIjdVqmQD\nHf78M3DffaFtu3ChVax+/vmZn82aZft97DEbD8mnenXLuQwYYEWfv/9esPQHEskcgc9TT1kntCee\nyH299HTr09CzpwXg7t0j08GPgSBIL7xgZYovvmh3Kp99Zp1KwmnOHDvGjTeGd79EBXHttdaLdvp0\nG6QtGKdO+dvQv/NO1s82bgTuucdaJeVU/zBsmHXImjMHOH7cLqAbN+b3G/j9+acVP0U6EMTFWT3B\nW28B+/YFXkfV+ip89pk1Ahk+3ALwuZGY0DeYcSjcfkRqrKHfflN9/HHVY8fs/fHjqv/3f6qDB6uW\nKaM6dKgtP3nSxgEaOdK/7ZIlgScAyS4tLet6hw+rbttm47tceKHqNdeE7esQhU16uo230717cOv/\n/LONv1O1qmr58qpHj/o/u/NO1ZgY+73lJCNDtUkTm0ipa1fbV+/eBfsOqjYhDaA6Y0bB9xWqn36y\nY0+YEPjzf//bPh8zxr+soOM+Icixhly/yAfziFQgePhhOyMPPmjv773X3levboO/bd/uX7dVK9WO\nHe31kSM2GFy/fnkfo2dPCyrjx6t+9JFqjRp2jA4d3PsHJQpG//6q9eoFt65vsLvJk7MOePj776oV\nKtgAd3l5+WX/oHAJCfYb891E7d6t+u239jh8OPjv8O23tr+FC4PfJpxat1Zt1CjwBf722y1wnjoV\nvuMxEOTDpZfadH0iqg88YGfngQcCrzt0qGqVKvYH/fJLW7dUKdWdO3Pef3q6/QjOPdf/D96kid0B\nVKxouYy0tML5bkQF5ZvSNJgL7113qZ5zjk19ed55qr162fLp020fX3+d9z7S0lTbtLFtkpNtu2ef\nVd2wwX5Hvt9QjRqqCxYE9x0SE22b1auDWz/cZsyw43/11ZmfNWum2qVLeI8XbCBgHYFj0yYrj3v6\naWvpMGGCtanOafz+pk2trHH3bv/EHydPWieSnPz4o1V+vfKKlZuOG2flqM8+a5XRa9ZY5RxRNGrc\n2J590zvm5vvvbbyd0qWteehHH9nESK+9BlxyiX+Au9xUqmTDXQwZYh3O2rWz7fv3t/3Om2eziFWp\nAnTrFlzzzPz2Kg6X/v2tg99rr2Vdfvy4ndcWLdxJl+t3+8E8IpEjeP55i9Tbtqn+8IPqX/5icwLk\nZNkyW//TTy3LfP75Vr5fp07OE177JhPPrWyUKFpt2WL/v5knfQ/k6FHLHT/0kL0/eNB+G747+IkT\n83d831wKgBWr+hw/bpPF16yZd5n6wIFWPxHO4pdQDRtmuf8//vAv+/57+16JieE9FpgjyFtqqt2p\nZGTY3UWLFsD559tcrwsWAPXq5bxtkyb2/NNP/rufu++2EUH/85/A23z5pY1R4tbdCFFBxMXZoGt5\nTWe5apXljlu1svcVK1pT0nvvtd/XLbfk7/h9+tgIqY88Atxwg395mTLWTn/nTmt+nZNFi6wV0siR\n+etVHC79+1v/CV/nUcA/r3hO44sVNs8GguPHbVCoHj2sGdvy5TlP7hJI5crWSWfpUmDrVvun797d\nlo0ff+b6GRkWCDLPE0tUlJQoYdMw5hUIfEWlvkAAWFHO5MlWBBrsaJ/ZxcT4p5bMzjdN5IIFgbc9\nfNiab158MfD44/k7fri0a2dDVSQm+petXGnnJbcBAQuTJwOBqrVjTkoCRoywuwh15vUNRbNmdpcB\nWI6gVCkbEvqrr2wArczWrQMOHGAgoKKtSRMLBJpLZ8pvvrE7/0C9d32z7eVXTttXr26BJ6dAMGqU\n5Rhef90CiptKlbJOYx9/bMODAzaER4sWBT8/+eW5QKAK/OtfVnH16KPW7XvtWisaytzDMRhNm9r+\nRPxZujvusB/Ak09mXffLL+2ZgYCKsiZNrEPU3r2BPz91ynoS+6bIjKRu3Sw34puBzOfzz4Fp04D7\n7/ePwuq2fv2sw9yiRdbJbc0a94qFAI8FghMnrAXCI49YSwZfd+/zzw89NwD4A0ejRsA559jrmBjL\nFSxb5u9av3cvMHu2FRvFxRXwSxC5yFc3tmYNMHOm1bFltnKlDSaX1yinhaFbN3teuNC/7MgRKxJq\n0CBwkZJbOnSw1k5z5tiNaHq6u4HA9RZBwTwK2mooLU113DjVunWtZv6xx8LTasDXe/LWW7MuP3rU\nWjAA1oGkYkXV0qVVp0wp+DGJ3LRnj/1fX3CBPcfEWLt+H19fgz17Ip+2jAzVWrVU4+KsFeDLL6u2\naGH9ggK123ebr8Oqr0/E5s3hPwbYocwcOmSdtgDVq68Ob4/C9HTV9u0Dd2b59Vf7UbRooXrDDdYh\nhqg4qFbNfk8jR1rnyLZt/U2m27dXbd7cvbQtXKjasqW/mWmTJqqvv+5eenJz7Jh1MOvSxUYpKOhw\nEoEEGwhEc6v1iRIJCQmalJQU8nYnT1pLnsWLrWLmuusKIXFEHvPhhzbDV48eNoLooEE2KONdd1nL\nl/vvd3/s/ZQUKwr2+jSuIrJSVRPyWi+PydaKtgcesLa6U6cyCBCFS+/e/te33GKBYdQom9oyPd2d\n+oHsWBcXmmJbWawK1KplU9/deafbqSEqnkRsnuFBg6zpZkyMzWxGRUuxzRGIAP/4h9upICr+ypSx\n5tgtW1rzUbfb6VPoim0gIKLIEbEhJKhoKrZFQ0REFBwGAiIij3MlEIjI/SLys4isFZE5IsJSRSIi\nl0Q8EIhILQAjACSoamMAJQEMiHQ6iIjIuFU0VApAWREpBaAcgJ0upYOIyPMiHghU9TcA4wH8CmAX\ngEOqujj7eiIyTESSRCQpNTU10skkIvIMN4qGzgXQA0A9ADUBnC0iN2dfT1WnqWqCqibExsZGOplE\nRJ7hRtHQNQC2qmqqqqYD+BBAEFNZExFRYXAjEPwKoLWIlBMRAdAJwHoX0kFERHCnjuA7AIkAfgCw\nxknDtEing4iIjCtDTKjq4wBcnkKaiIgA9iwmIvI8BgIiIo9jICAi8jgGAiIij2MgICLyOAYCIiKP\nYyAgIvI4BgIiIo9jICAi8jgGAiIij2MgICLyOAYCIiKPYyAgIvI4V0YfJSLvSk9Px44dO3D8+HG3\nk1JsxMTEoHbt2ihdunS+tmcgIKKI2rFjBypUqIC4uDjY3FRUEKqK/fv3Y8eOHahXr16+9sGiISKK\nqOPHj6NKlSoMAmEiIqhSpUqBclgMBEQUcQwC4VXQ88lAQESeU7JkScTHx6Nx48bo168fjh49mu99\nLVu2DN26dQMAfPTRRxg3blyO6x48eBBTpkw5/X7nzp3o27dvvo8dLkEFAjE3i8hjzvvzRaRV4SaN\niKhwlC1bFqtXr8batWtx1llnYerUqVk+V1VkZGSEvN/u3btjzJgxOX6ePRDUrFkTiYmJIR8n3ILN\nEUwB0AbAQOf97wBeKZQUEREvMU3eAAARsklEQVRF0FVXXYXNmzcjJSUFDRs2xK233orGjRtj+/bt\nWLx4Mdq0aYMWLVqgX79+OHLkCADg008/RaNGjdCiRQt8+OGHp/c1c+ZMDB8+HACwZ88e9OrVC82a\nNUOzZs3w7bffYsyYMdiyZQvi4+MxatQopKSkoHHjxgCs7uS2225DkyZN0Lx5c3zxxRen99m7d290\n7doVDRo0wOjRo8N+DoJtNXS5qrYQkVUAoKppInJW2FNDRN4yciSwenV49xkfD0ycGNSqJ0+exCef\nfIKuXbsCADZt2oRZs2ahdevW2LdvH55++mksWbIEZ599Np577jlMmDABo0ePxh133IGlS5eifv36\n6N+/f8B9jxgxAu3bt8fcuXNx6tQpHDlyBOPGjcPatWux2vnOKSkpp9d/5ZVXICJYs2YNkpOTce21\n12Ljxo0AgNWrV2PVqlUoU6YMGjZsiL/97W+oU6dOAU5SVsHmCNJFpCQABQARiQUQer6JiCgKHDt2\nDPHx8UhISMD555+PIUOGAADq1q2L1q1bAwBWrFiBdevWoW3btoiPj8esWbOwbds2JCcno169emjQ\noAFEBDfffHPAYyxduhR33303AKuTqFixYq5p+vrrr0/vq1GjRqhbt+7pQNCpUydUrFgRMTExuOSS\nS7Bt27awnAefYHMELwOYC6CaiDwDoC+AR8KaEiLyniDv3MPNV0eQ3dlnn336taqic+fOmDNnTpZ1\nAm1X2MqUKXP6dcmSJXHy5Mmw7j+oHIGqzgYwGsCzAHYB6Kmq74c1JUREUaR169b45ptvsHnzZgDA\nH3/8gY0bN6JRo0ZISUnBli1bAOCMQOHTqVMnvPrqqwCAU6dO4dChQ6hQoQJ+//33gOtfddVVmD17\nNgBg48aN+PXXX9GwYcNwf62Agm01dCGArar6CoC1ADqLSKVCTRkRkYtiY2Mxc+ZMDBw4EE2bNkWb\nNm2QnJyMmJgYTJs2DX/5y1/QokULVKtWLeD2kyZNwhdffIEmTZrgsssuw7p161ClShW0bdsWjRs3\nxqhRo7Ksf8899yAjIwNNmjRB//79MXPmzCw5gcIkqpr3SiKrASQAiAPwHwAfAbhUVa8v1NQ5EhIS\nNCkpKRKHIqJCtn79elx88cVuJ6PYCXReRWSlqibktW2wlcUZqnoSQG8Ak1V1FIAaIaeUiIiiTiit\nhgYCuBXAAmdZ/oa5IyKiqBJsILgN1qHsGVXdKiL1ALxVeMkiIqJICbbV0DoAfwewRkQaA9ihqs/l\n96AiUklEEkUkWUTWi0ib/O6LiIgKJqh+BCLSAcAsACkABEAdERmkql/m87iTAHyqqn2dHsrl8rkf\nIiIqoGA7lL0I4FpV3QAAInIRgDkALgv1gCJSEUA7AIMBQFVPADgR6n6IiCg8gq0jKO0LAgCgqhuR\n/8riegBSAbwhIqtEZLqInJ19JREZJiJJIpKUmpqaz0MREQU2b948iAiSk5NzXW/mzJnYuXNnvo+T\neZjqaBVsIEhyLtgdnMdrAPLbsL8UgBYAXlXV5gD+AHDGuK2qOk1VE1Q1ITY2Np+HIiIKbM6cObjy\nyitz7BnsU9BAUBQEGwjuBrAOwAjnsc5Zlh87YJXN3znvE2GBgYgoIo4cOYKvv/4aM2bMwLvvvnt6\n+XPPPYcmTZqgWbNmGDNmDBITE5GUlISbbroJ8fHxOHbsGOLi4rBv3z4AQFJSEjp06AAA+P7779Gm\nTRs0b94cV1xxBTZs2BDo0FEpqDoCVf0TwATnUSCqultEtotIQ6e4qRMssBCRx7g1CvX8+fPRtWtX\nXHTRRahSpQpWrlyJvXv3Yv78+fjuu+9Qrlw5HDhwAJUrV8bkyZMxfvx4JCTk3kG3UaNG+Oqrr1Cq\nVCksWbIEDz30ED744IMwfrPCk2sgEJE1cIaeDkRVm+bzuH8DMNtpMfQLrJ8CEVFEzJkzB/fddx8A\nYMCAAZgzZw5UFbfddhvKlbNGjJUrVw5pn4cOHcKgQYOwadMmiAjS09PDnu7CkleOoDeA6gC2Z1te\nB8Du/B5UVX1jFxGRh7kxCvWBAwewdOlSrFmzBiKCU6dOQUTQr1+/oLYvVarU6Wksjx8/fnr5o48+\niquvvhpz585FSkrK6SKjoiCvOoKXABxS1W2ZHwAOOZ8RERUpiYmJuOWWW7Bt2zakpKRg+/btqFev\nHipWrIg33njj9ET2Bw4cAIAzho6Oi4vDypUrASBL0c+hQ4dQq1YtAFbBXJTkFQiqq+qa7AudZXGF\nkiIiokI0Z84c9OrVK8uyPn36YNeuXejevTsSEhIQHx+P8ePHAwAGDx6Mu+6663Rl8eOPP4777rsP\nCQkJKFmy5Ol9jB49GmPHjkXz5s3DPnFMYct1GGoR2aSqDXL4bLOq1i+0lGXCYaiJig8OQ104CnMY\n6iQRuSP7QhEZCmBlSKkkIqKolFdl8UgAc0XkJvgv/AkAzgLQK8etiIioyMg1EKjqHgBXiMjVABo7\ni/+jqksLPWVERBQRwXYo+wLAF4WcFiLyCFWFiLidjGIjmCmHcxPsEBNERGERExOD/fv3F/jiRUZV\nsX//fsTExOR7H8EOQ01EFBa1a9fGjh07wFGFwycmJga1a9fO9/YMBEQUUaVLl0a9evXcTgZlwqIh\nIiKPYyAgIvI4BgIiIo9jICAi8jgGAiIij2MgICLyOAYCIiKPYyAgIvI4BgIiIo9jICAi8jgGAiIi\nj2MgICLyOAYCIiKPYyAgIvI4BgIiIo9jICAi8jgGAiIij2MgICLyOAYCIiKPYyAgIvI41wKBiJQU\nkVUissCtNBARkbs5gvsArHfx+EREBJcCgYjUBvAXANPdOD4REfm5lSOYCGA0gIycVhCRYSKSJCJJ\nqampkUsZEZHHRDwQiEg3AHtVdWVu66nqNFVNUNWE2NjYCKWOiMh73MgRtAXQXURSALwLoKOIvO1C\nOoiICC4EAlUdq6q1VTUOwAAAS1X15king4iIDPsREBF5XCk3D66qywAsczMNRERexxwBEZHHMRAQ\nEXkcAwERkccxEBAReRwDARGRxzEQEBF5HAMBEZHHMRAQEXkcAwERkccxEBAReRwDARGRxzEQEBF5\nHAMBEZHHMRAQEXkcAwERkccxEBAReRwDARGRxzEQEBF5HAMBEZHHMRAQEXkcAwERkccxEBAReRwD\nARGRxzEQEBF5HAMBEZHHMRAQEXkcAwERkccxEBAReRwDARGRx0U8EIhIHRH5QkTWicjPInJfpNNA\nRER+pVw45kkAD6rqDyJSAcBKEflMVde5kBYiIs+LeI5AVXep6g/O698BrAdQK9LpICIi42odgYjE\nAWgO4LsAnw0TkSQRSUpNTY100oiIPMO1QCAi5QF8AGCkqh7O/rmqTlPVBFVNiI2NjXwCiYg8wpVA\nICKlYUFgtqp+6EYaiIjIuNFqSADMALBeVSdE+vhERJSVGzmCtgBuAdBRRFY7j+tdSAcREcGF5qOq\n+jUAifRxiYgoMPYsJiLyOAYCIiKPYyAgIvI4BgIiIo9jICAi8jgGAiIij2MgICLyOAYCIiKPYyAg\nIvI4BgIiIo9jICAi8jgGAiIij3NjzuLIGTwYWLrU7VREnnhwTD9+Z2/w4nf+7DPgwgsL9RDFOxDE\nxwMlPJbpUXU7BZHH7+wNXvzOAFC2bKEfongHgpEj3U4BEVHU89jtMhERZcdAQETkcQwEREQex0BA\nRORxDARERB7HQEBE5HEMBEREHsdAQETkcaJFoLeeiKQC2JbPzasC2BfG5BQGpjE8ikIagaKRTqYx\nPNxOY11Vjc1rpSIRCApCRJJUNcHtdOSGaQyPopBGoGikk2kMj6KQRoBFQ0REnsdAQETkcV4IBNPc\nTkAQmMbwKAppBIpGOpnG8CgKaSz+dQRERJQ7L+QIiIgoFwwEREQeV6wDgYh0FZENIrJZRMa4nR4A\nEJE6IvKFiKwTkZ9F5D5neWUR+UxENjnP57qczpIiskpEFjjv64nId865/LeInOVm+pw0VRKRRBFJ\nFpH1ItImCs/j/c7fea2IzBGRGLfPpYi8LiJ7RWRtpmUBz5uYl520/iQiLVxM4wvO3/onEZkrIpUy\nfTbWSeMGEekSiTTmlM5Mnz0oIioiVZ33rpzLYBTbQCAiJQG8AuA6AJcAGCgil7ibKgDASQAPquol\nAFoDuNdJ1xgAn6tqAwCfO+/ddB+A9ZnePwfgJVWtDyANwBBXUpXVJACfqmojAM1g6Y2a8ygitQCM\nAJCgqo0BlAQwAO6fy5kAumZbltN5uw5AA+cxDMCrLqbxMwCNVbUpgI0AxgKA8/sZAOBSZ5spzu/f\nrXRCROoAuBbAr5kWu3Uu81RsAwGAVgA2q+ovqnoCwLsAericJqjqLlX9wXn9O+ziVQuWtlnOarMA\n9HQnhYCI1AbwFwDTnfcCoCOARGcVV9MHACJSEUA7ADMAQFVPqOpBRNF5dJQCUFZESgEoB2AXXD6X\nqvolgAPZFud03noAeFPNCgCVRKSGG2lU1cWqetJ5uwJA7UxpfFdV/1TVrQA2w37/hS6HcwkALwEY\nDSBzaxxXzmUwinMgqAVge6b3O5xlUUNE4gA0B/AdgOqqusv5aDeA6i4lCwAmwv6JM5z3VQAczPQj\njIZzWQ9AKoA3nCKs6SJyNqLoPKrqbwDGw+4KdwE4BGAlou9cAjmft2j9Hd0O4BPndVSlUUR6APhN\nVX/M9lFUpTOz4hwIopqIlAfwAYCRqno482dqbXpdadcrIt0A7FXVlW4cPwSlALQA8KqqNgfwB7IV\nA7l5HgHAKWfvAQtaNQGcjQDFCNHG7fOWFxF5GFbEOtvttGQnIuUAPATgMbfTEoriHAh+A1An0/va\nzjLXiUhpWBCYraofOov3+LKJzvNel5LXFkB3EUmBFad1hJXFV3KKN4DoOJc7AOxQ1e+c94mwwBAt\n5xEArgGwVVVTVTUdwIew8xtt5xLI+bxF1e9IRAYD6AbgJvV3goqmNF4IC/w/Or+h2gB+EJHzEF3p\nzKI4B4L/AWjgtNA4C1aZ9JHLafKVt88AsF5VJ2T66CMAg5zXgwDMj3TaAEBVx6pqbVWNg52zpap6\nE4AvAPR1O30+qrobwHYRaegs6gRgHaLkPDp+BdBaRMo5f3dfGqPqXDpyOm8fAbjVafHSGsChTEVI\nESUiXWFFlt1V9Wimjz4CMEBEyohIPVhl7PdupFFV16hqNVWNc35DOwC0cP5fo+ZcnkFVi+0DwPWw\n1gVbADzsdnqcNF0Jy3b/BGC187geVg7/OYBNAJYAqBwFae0AYIHz+gLYj2szgPcBlImC9MUDSHLO\n5TwA50bbeQTwJIBkAGsBvAWgjNvnEsAcWJ1FOuxCNSSn8wZAYK3vtgBYA2sB5VYaN8PK2H2/m6mZ\n1n/YSeMGANe5eS6zfZ4CoKqb5zKYB4eYICLyuOJcNEREREFgICAi8jgGAiIij2MgICLyOAYCIiKP\nK5X3KkTeIiKnYM37SsN6sL4JGyQuI9cNiYooBgKiMx1T1XgAEJFqAN4BcA6Ax11NFVEhYdEQUS5U\ndS9syODhTo/QOBH5SkR+cB5XAICIvCkip0cRFZHZItJDRC4Vke9FZLUzBn0Dt74LUU7YoYwoGxE5\noqrlsy07CKAhgN8BZKjqceeiPkdVE0SkPYD7VbWnM0T2athQBy8BWKGqs52hTkqq6rHIfiOi3LFo\niCg0pQFMFpF4AKcAXAQAqvpfEZkiIrEA+gD4QFVPishyAA87czx8qKqbXEs5UQ5YNESUBxG5AHbR\n3wvgfgB7YDOiJQDIPM3kmwBuBnAbgNcBQFXfAdAdwDEAC0WkY+RSThQc5giIcuHc4U8FMFlV1Sn2\n2aGqGSIyCDb9pM9M2GByu1V1nbP9BQB+UdWXReR8AE0BLI3olyDKAwMB0ZnKishq+JuPvgXAN2T4\nFAAfiMitAD6FTYgDAFDVPSKyHjYSqs+NAG4RkXTYzF//ikD6iULCymKiMHFmp1oDG3/+kNvpIQoW\n6wiIwkBErgGwHsD/MQhQUcMcARGRxzFHQETkcQwEREQex0BARORxDARERB7HQEBE5HH/H7alHFzR\ni2gvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newp,newy_test = plot_result(stock_name, p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "8da9fd48928505f0503fef813602984acc78beb7",
    "colab": {},
    "colab_type": "code",
    "id": "g4q4ECA3BEB1"
   },
   "outputs": [],
   "source": [
    "my_pred = pd.DataFrame(columns=['pred', 'actual'])\n",
    "my_pred.head()\n",
    "\n",
    "my_pred['pred'] = newp.tolist()\n",
    "my_pred['actual'] = newy_test.tolist()\n",
    "\n",
    "my_pred.head()\n",
    "\n",
    "my_pred.to_csv('LSTM_Stock_prediction_Close.csv', sep=',', encoding ='utf-8')\n",
    "\n",
    "model.save('LSTM_Stock_prediction_Close.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "47ca246a8d26dde571a583078044252acf675999",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FjbsREH3BEB3",
    "outputId": "1e7c512f-069b-44e8-a8ce-d86a0b75fbf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149,), (149,))"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, my_pred['pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "45fad16b92ebc4a4e16555311c1310b86ec2a295",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "z0HEMeBQBEB5",
    "outputId": "ec6baa17-e3b3-4056-dde1-ff5f3edd116b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0253956317901611]</td>\n",
       "      <td>[8.418510250096968]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.0253684520721436]</td>\n",
       "      <td>[8.544661442008255]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0253374576568604]</td>\n",
       "      <td>[8.58971666674878]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.0252994298934937]</td>\n",
       "      <td>[8.67982539754874]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.0252554416656494]</td>\n",
       "      <td>[8.814987634408132]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pred               actual\n",
       "0  [1.0253956317901611]  [8.418510250096968]\n",
       "1  [1.0253684520721436]  [8.544661442008255]\n",
       "2  [1.0253374576568604]   [8.58971666674878]\n",
       "3  [1.0252994298934937]   [8.67982539754874]\n",
       "4  [1.0252554416656494]  [8.814987634408132]"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "1f888e28faa894080b4da014a6b51bd082a9ed6a",
    "colab": {},
    "colab_type": "code",
    "id": "NqcrjZmJBEB7"
   },
   "outputs": [],
   "source": [
    "#my_pred_1 = pd.DataFrame(columns=['pred', 'actual'])\n",
    "#my_pred_1.head()\n",
    "\n",
    "#my_pred_1['pred'] = newp\n",
    "#my_pred_1['actual'] = newy_test\n",
    "\n",
    "#my_pred.head()\n",
    "print(newp, newy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "d07ee739170b0ec5cdb6bf6ea83b3b23d9faa520",
    "colab": {},
    "colab_type": "code",
    "id": "q_67YsQvBEB9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_pred_data = np.concatenate((newy_test, newp), axis=1)\n",
    "print(my_pred_data.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "290fa6f93f9477a1201802e058cac4f81deac585",
    "colab": {},
    "colab_type": "code",
    "id": "NuQgq9SeBEB_"
   },
   "outputs": [],
   "source": [
    "from statsmodels import robust\n",
    "along_row = robust.mad(my_pred_data, axis=1)\n",
    "along_column = robust.mad(my_pred_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "2a7289f1de9393fb987c69649d17a01cb2acc371",
    "colab": {},
    "colab_type": "code",
    "id": "ynk8acx1BECA"
   },
   "outputs": [],
   "source": [
    "print(along_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "2bbb43906622f6f13b4fe6a16cbbafd8262d4295",
    "colab": {},
    "colab_type": "code",
    "id": "QBOzIoC9BECC"
   },
   "outputs": [],
   "source": [
    "print(along_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_uuid": "6394fc7586eeb6973333a606e5072c74b8588a17",
    "colab": {},
    "colab_type": "code",
    "id": "mAtUA-trBECE"
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"LSTM_col_y-test_pred.csv\", my_pred_data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y6xzWlYjU0_3"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b06LR3wuU10t"
   },
   "outputs": [],
   "source": [
    "def build_model3(layers, neurons, rate, decay):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(rate))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(rate))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aTgNb3vU2em"
   },
   "outputs": [],
   "source": [
    "\n",
    "def quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model3(shape, neurons, rate, decay)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"/content/drive/My Drive/abc/model.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    model.save('/content/drive/My Drive/abc/LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-KWIPzNU3Lm"
   },
   "outputs": [],
   "source": [
    "stock_name = 'INFY'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [512, 512, 64, 1]\n",
    "epochs = 90\n",
    "rate = 0.3 #dropout\n",
    "decay = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3485
    },
    "colab_type": "code",
    "id": "3LC1Q8AeU37y",
    "outputId": "7c69a20a-d583-4dd6-ebb4-e1ced892fabe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_40 (LSTM)               (None, 22, 512)           1058816   \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 22, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,190,913\n",
      "Trainable params: 3,190,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1204 samples, validate on 134 samples\n",
      "Epoch 1/90\n",
      "1204/1204 [==============================] - 8s 7ms/step - loss: 0.2118 - acc: 8.3056e-04 - val_loss: 0.0634 - val_acc: 0.0000e+00\n",
      "Epoch 2/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0412 - acc: 8.3056e-04 - val_loss: 0.0631 - val_acc: 0.0000e+00\n",
      "Epoch 3/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0234 - acc: 8.3056e-04 - val_loss: 0.0033 - val_acc: 0.0000e+00\n",
      "Epoch 4/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0197 - acc: 8.3056e-04 - val_loss: 0.0097 - val_acc: 0.0000e+00\n",
      "Epoch 5/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0121 - acc: 8.3056e-04 - val_loss: 0.0334 - val_acc: 0.0000e+00\n",
      "Epoch 6/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0145 - acc: 8.3056e-04 - val_loss: 0.0066 - val_acc: 0.0000e+00\n",
      "Epoch 7/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0097 - acc: 8.3056e-04 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 8/90\n",
      "1204/1204 [==============================] - 0s 268us/step - loss: 0.0084 - acc: 8.3056e-04 - val_loss: 0.0140 - val_acc: 0.0000e+00\n",
      "Epoch 9/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0082 - acc: 8.3056e-04 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 10/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0051 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 11/90\n",
      "1204/1204 [==============================] - 0s 267us/step - loss: 0.0046 - acc: 8.3056e-04 - val_loss: 0.0074 - val_acc: 0.0000e+00\n",
      "Epoch 12/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0047 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 13/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0042 - acc: 8.3056e-04 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 14/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0036 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 15/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0033 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 16/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 17/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0031 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 18/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0030 - acc: 8.3056e-04 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 19/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 20/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 21/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 22/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0029 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 23/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0028 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 24/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 25/90\n",
      "1204/1204 [==============================] - 0s 270us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 26/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 27/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 28/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 29/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 30/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 31/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 32/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0025 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 33/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 34/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0024 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 35/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 36/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 37/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 38/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 39/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 40/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 41/90\n",
      "1204/1204 [==============================] - 0s 287us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 42/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 43/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 44/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 45/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 46/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 47/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 48/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 49/90\n",
      "1204/1204 [==============================] - 0s 278us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 50/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 51/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 52/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 53/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0016 - val_acc: 0.0000e+00\n",
      "Epoch 54/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 55/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 56/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 57/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 58/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0027 - acc: 8.3056e-04 - val_loss: 0.0021 - val_acc: 0.0000e+00\n",
      "Epoch 59/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0026 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 60/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0023 - acc: 8.3056e-04 - val_loss: 0.0027 - val_acc: 0.0000e+00\n",
      "Epoch 61/90\n",
      "1204/1204 [==============================] - 0s 273us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 62/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0022 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 63/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0021 - acc: 8.3056e-04 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 64/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 65/90\n",
      "1204/1204 [==============================] - 0s 286us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 66/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 67/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0020 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 68/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 69/90\n",
      "1204/1204 [==============================] - 0s 282us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 70/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 71/90\n",
      "1204/1204 [==============================] - 0s 271us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.8575e-04 - val_acc: 0.0000e+00\n",
      "Epoch 72/90\n",
      "1204/1204 [==============================] - 0s 288us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 73/90\n",
      "1204/1204 [==============================] - 0s 274us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 74/90\n",
      "1204/1204 [==============================] - 0s 276us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 75/90\n",
      "1204/1204 [==============================] - 0s 285us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 76/90\n",
      "1204/1204 [==============================] - 0s 269us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 9.2453e-04 - val_acc: 0.0000e+00\n",
      "Epoch 77/90\n",
      "1204/1204 [==============================] - 0s 277us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 78/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.1692e-04 - val_acc: 0.0000e+00\n",
      "Epoch 79/90\n",
      "1204/1204 [==============================] - 0s 280us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 9.5262e-04 - val_acc: 0.0000e+00\n",
      "Epoch 80/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.9669e-04 - val_acc: 0.0000e+00\n",
      "Epoch 81/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0016 - acc: 8.3056e-04 - val_loss: 8.8435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 82/90\n",
      "1204/1204 [==============================] - 0s 284us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 83/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 84/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 85/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 86/90\n",
      "1204/1204 [==============================] - 0s 272us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 8.3558e-04 - val_acc: 0.0000e+00\n",
      "Epoch 87/90\n",
      "1204/1204 [==============================] - 0s 275us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 88/90\n",
      "1204/1204 [==============================] - 0s 281us/step - loss: 0.0017 - acc: 8.3056e-04 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 89/90\n",
      "1204/1204 [==============================] - 0s 283us/step - loss: 0.0018 - acc: 8.3056e-04 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 90/90\n",
      "1204/1204 [==============================] - 0s 279us/step - loss: 0.0019 - acc: 8.3056e-04 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Train Score: 0.00128 MSE (0.04 RMSE)\n",
      "Test Score: 0.00230 MSE (0.05 RMSE)\n"
     ]
    }
   ],
   "source": [
    "trainScore, testScore = quick_measure(stock_name, seq_len, rate, shape, neurons, epochs, decay)\n",
    "seq_len_result[seq_len] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLp2RuivU5TG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load json and create model\n",
    "json_file = open('/content/drive/My Drive/abc/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/content/drive/My Drive/abc/LSTM_Stock_prediction-20170429.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jzU6iPrHU4p0"
   },
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvzjPAitaP3e"
   },
   "outputs": [],
   "source": [
    "p = percentage_difference(loaded_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "erP7Xj4gZlBi",
    "outputId": "5296aa86-a3fc-4dee-e34e-7936d506a296"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncrIwM7OmnCzMzKwpJwszM2vqzwPxDF3JktnIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "newp,newy_test = plot_result(stock_name, p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "v-VGS_vwaiU_",
    "outputId": "dc7f5c76-5b83-4996-f3a3-3f9dbf943cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00128 MSE (0.04 RMSE)\n",
      "Test Score: 0.00230 MSE (0.05 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0012843664479755166, 0.0023036579563963733)"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(loaded_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-KK4sALibHD9",
    "outputId": "60992198-24db-48a1-bc6e-984981134801"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(newp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "r51-VhMpb1xf",
    "outputId": "31dbb07f-bd26-46fa-db10-7336cf119d34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149, 1), array([ 8.014532 ,  8.131971 ,  8.224032 ,  8.304104 ,  8.372955 ,\n",
       "         8.440688 ,  8.484501 ,  8.545276 ,  8.614725 ,  8.680065 ,\n",
       "         8.741358 ,  8.813691 ,  8.882997 ,  8.859953 ,  8.835779 ,\n",
       "         8.788089 ,  8.705655 ,  8.612552 ,  8.531593 ,  8.5117035,\n",
       "         8.543737 ,  8.602729 ,  8.663547 ,  8.743856 ,  8.881939 ,\n",
       "         9.056303 ,  9.252837 ,  9.352019 ,  9.379025 ,  9.38559  ,\n",
       "         9.413256 ,  9.457051 ,  9.521111 ,  9.525675 ,  9.542867 ,\n",
       "         9.572714 ,  9.604424 ,  9.616242 ,  9.666911 ,  9.743256 ,\n",
       "         9.856785 ,  9.994585 , 10.121975 , 10.211    , 10.21419  ,\n",
       "        10.180416 , 10.150592 , 10.126975 , 10.131687 , 10.160402 ,\n",
       "        10.205768 , 10.27153  , 10.36438  , 10.438131 , 10.503021 ,\n",
       "        10.551326 , 10.572066 , 10.601438 , 10.636182 , 10.598599 ,\n",
       "        10.53796  , 10.463917 , 10.398114 , 10.324223 , 10.298692 ,\n",
       "        10.309382 , 10.304574 , 10.292196 , 10.316877 , 10.365255 ,\n",
       "        10.389481 , 10.417091 , 10.461898 , 10.493402 , 10.529792 ,\n",
       "        10.598495 , 10.672794 , 10.691392 , 10.633214 , 10.518171 ,\n",
       "        10.379877 , 10.239836 , 10.101555 ,  9.994958 ,  9.953926 ,\n",
       "         9.950546 ,  9.977484 , 10.034956 , 10.141895 , 10.236229 ,\n",
       "        10.254981 , 10.1844425, 10.126956 , 10.05642  ,  9.990799 ,\n",
       "         9.875551 ,  9.69971  ,  9.554705 ,  9.5008745,  9.578705 ,\n",
       "         9.653493 ,  9.635924 ,  9.574266 ,  9.464829 ,  9.299    ,\n",
       "         9.09391  ,  8.879632 ,  8.659899 ,  8.479265 ,  8.397052 ,\n",
       "         8.454872 ,  8.546373 ,  8.6394825,  8.731459 ,  8.831505 ,\n",
       "         8.960321 ,  9.064592 ,  9.109827 ,  9.095351 ,  9.053326 ,\n",
       "         8.980618 ,  8.89623  ,  8.827226 ,  8.773386 ,  8.694896 ,\n",
       "         8.591064 ,  8.504687 ,  8.4520235,  8.459043 ,  8.623413 ,\n",
       "         8.849509 ,  9.098968 ,  9.3412   ,  9.540689 ,  9.649438 ,\n",
       "         9.68576  ,  9.610081 ,  9.526097 ,  9.461281 ,  9.467539 ,\n",
       "         9.521831 ,  9.530406 ,  9.492033 ,  9.396726 ,  9.256157 ,\n",
       "         9.066373 ,  8.871754 ,  8.735672 ,  8.683884 ], dtype=float32))"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newp.shape,newp.reshape(149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Nl-bW5-zbeq4",
    "outputId": "006f9166-217f-42a5-9ada-88252ccba4f5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.014532</td>\n",
       "      <td>8.418510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.131971</td>\n",
       "      <td>8.544661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.224032</td>\n",
       "      <td>8.589717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.304104</td>\n",
       "      <td>8.679825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.372955</td>\n",
       "      <td>8.814988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pred    actual\n",
       "0  8.014532  8.418510\n",
       "1  8.131971  8.544661\n",
       "2  8.224032  8.589717\n",
       "3  8.304104  8.679825\n",
       "4  8.372955  8.814988"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pred = pd.DataFrame(columns=['pred', 'actual'])\n",
    "my_pred.head()\n",
    "\n",
    "my_pred['pred'] = newp.reshape(149)\n",
    "my_pred['actual'] = newy_test.reshape(149)\n",
    "\n",
    "my_pred.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JanSnT8dJKF"
   },
   "outputs": [],
   "source": [
    "my_pred.to_csv('/content/drive/My Drive/abc/LSTM_Stock_prediction_Close.csv', sep=',', encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QuuSOOZEd3Y9",
    "outputId": "a8071571-0b2a-412d-d39e-77d0a6c9077e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1510, 4), (1338, 22, 4), (149, 22, 4))"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sN_LGh_eJhb"
   },
   "outputs": [],
   "source": [
    "df.to_csv('/content/drive/My Drive/abc/dataset.csv', sep=',', encoding ='utf-8')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "lstm-for-closing-price.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
